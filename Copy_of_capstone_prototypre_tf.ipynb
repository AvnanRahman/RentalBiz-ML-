{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michinsaramiya/RentalBiz-ML-/blob/main/Copy_of_capstone_prototypre_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b8305b3",
      "metadata": {
        "id": "8b8305b3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33896b60",
      "metadata": {
        "id": "33896b60",
        "outputId": "410fb71b-c0a0-48d8-925b-652aa2fdae5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b3f0cd69-9a55-4b93-90f9-6bcfcbdff00c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b3f0cd69-9a55-4b93-90f9-6bcfcbdff00c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving OnlineRetailfix.csv to OnlineRetailfix.csv\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(next(iter(uploaded.keys())), encoding='latin-1')\n"
      ],
      "metadata": {
        "id": "4tH0ZiJQQKJd"
      },
      "id": "4tH0ZiJQQKJd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cb96bd",
      "metadata": {
        "scrolled": true,
        "id": "a1cb96bd",
        "outputId": "69193557-443f-4392-db04-9260a1c4f495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1106 rows and 7 columns\n",
            "\n",
            "====================================================\n",
            "Missing Values:\n",
            " InvoiceNo         0\n",
            "Description       0\n",
            "UnitPrice         0\n",
            "CustomerID        0\n",
            "Country           0\n",
            "Category          0\n",
            "Unnamed: 6     1106\n",
            "dtype: int64\n",
            "\n",
            "====================================================\n",
            "Categories Counts:\n",
            " Hobi            324\n",
            "Dapur           193\n",
            "Cafe            169\n",
            "Baju            143\n",
            "Elektronik      115\n",
            "Travel          102\n",
            "Musik            20\n",
            "Transportasi     20\n",
            "Kamera           20\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Dataset information\n",
        "print(\"There are {} rows and {} columns\".format(df.shape[0],df.shape[1]))\n",
        "print(\"\\n====================================================\")\n",
        "print(\"Missing Values:\\n\", df.isnull().sum())\n",
        "print(\"\\n====================================================\")\n",
        "print(\"Categories Counts:\\n\", df['Category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002c58fc",
      "metadata": {
        "id": "002c58fc"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['Description'], df['Category'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873cfa17",
      "metadata": {
        "id": "873cfa17"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the text\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e36819",
      "metadata": {
        "scrolled": true,
        "id": "18e36819",
        "outputId": "ed6f972c-004c-413a-8d95-d8b932fe00f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "987\n",
            "{'<OOV>': 1, 'set': 2, 'red': 3, 'of': 4, 'heart': 5, 'retrospot': 6, 'white': 7, 'christmas': 8, 'bag': 9, 't': 10, 'vintage': 11, 'warmer': 12, 'water': 13, 'bottle': 14, 'hand': 15, 'hot': 16, 'design': 17, 'light': 18, 'cake': 19, 'pack': 20, 'holder': 21, 'pink': 22, 'box': 23, 'black': 24, 'blue': 25, 'small': 26, '6': 27, '3': 28, 'cases': 29, 'metal': 30, 'polkadot': 31, 'mug': 32, 'tea': 33, 'with': 34, 'kit': 35, 'jumbo': 36, 'lights': 37, '2': 38, 'union': 39, 'jam': 40, 'in': 41, 'paper': 42, 'coffee': 43, 'hanging': 44, 'lunch': 45, 'wooden': 46, 'glass': 47, 'feltcraft': 48, 'jack': 49, 'bird': 50, 'assorted': 51, 'kitchen': 52, 'silver': 53, 'paisley': 54, 'and': 55, 'clock': 56, 'love': 57, 'sign': 58, 'making': 59, '60': 60, 'fairy': 61, 'cream': 62, 'boxes': 63, 'tin': 64, 'frame': 65, 'card': 66, 'ribbons': 67, '12': 68, 'star': 69, 'babushka': 70, 'wood': 71, 'spaceboy': 72, 'edwardian': 73, 'cosy': 74, 'party': 75, 'candles': 76, 'magic': 77, 'wicker': 78, 'chain': 79, '10': 80, 'drawer': 81, 'dolly': 82, 'girl': 83, 'dog': 84, 'colour': 85, 'decoration': 86, 'garden': 87, 'ivory': 88, 'mini': 89, '72': 90, 'green': 91, 'printed': 92, 'giant': 93, 'candle': 94, 'hearts': 95, 'coat': 96, 'hanger': 97, 'rose': 98, 'finish': 99, 'ceramic': 100, '5': 101, 'tube': 102, 'wall': 103, 'lantern': 104, 'tissues': 105, 'skittles': 106, 'new': 107, 'matches': 108, 'ribbon': 109, 'reel': 110, 'chocolate': 111, 'canon': 112, 'scented': 113, 'jars': 114, 'building': 115, 'block': 116, 'word': 117, 'billboard': 118, 'bank': 119, 'skull': 120, 'toadstool': 121, 'towels': 122, 'umbrella': 123, 'doormat': 124, 'large': 125, 'plasters': 126, 'alarm': 127, 'bakelike': 128, 'i': 129, 'london': 130, 'slate': 131, 'woolly': 132, 'hottie': 133, 's': 134, 'skulls': 135, 'bike': 136, 'the': 137, '9': 138, 'cabinet': 139, '7': 140, 'knob': 141, 'acrylic': 142, 'antique': 143, 'parasol': 144, 'sweetheart': 145, 'enamel': 146, 'owl': 147, 'drawing': 148, 'doll': 149, 'casual': 150, 'gingham': 151, 'women': 152, 'fashion': 153, 'retro': 154, 'traditional': 155, '4': 156, 'mountain': 157, 'powershot': 158, 'baroque': 159, 'me': 160, \"50's\": 161, 'money': 162, 'popcorn': 163, 'men': 164, 'shirt': 165, 'circus': 166, 'parade': 167, 'home': 168, 'grey': 169, 'hook': 170, 'scandinavian': 171, 'sewing': 172, 'scotty': 173, 'soldier': 174, 'strawberry': 175, 'dot': 176, '20': 177, 'english': 178, 'photo': 179, 'slipper': 180, 'fit': 181, 'thermometer': 182, 'cup': 183, 'cookie': 184, 'nesting': 185, 'hd': 186, 'summer': 187, 'led': 188, 'woodland': 189, 'yamaha': 190, 'guitar': 191, 'ornament': 192, 'polka': 193, 'princess': 194, 'pencils': 195, 'fujifilm': 196, 'cornice': 197, 'tier': 198, 'reds': 199, 'paris': 200, 'charlotte': 201, 'shoes': 202, 'regular': 203, 'diner': 204, 'cotton': 205, 'cupid': 206, 'frosted': 207, 'round': 208, 'snack': 209, 'snowmen': 210, 'sympathy': 211, 'bath': 212, 'drink': 213, 'piece': 214, 'tv': 215, 'size': 216, 'tops': 217, 'tree': 218, 'ant': 219, 'filigree': 220, 'notebook': 221, 'shopper': 222, 'night': 223, 'bucket': 224, 'pot': 225, 'plate': 226, 'luggage': 227, 'casio': 228, 'exilim': 229, 'ex': 230, 'cutlery': 231, 'zinc': 232, 'homemade': 233, 'lola': 234, 'french': 235, 'travel': 236, 'folding': 237, 'rotating': 238, 'suki': 239, 'napkins': 240, 'jigsaw': 241, 'egg': 242, 'battery': 243, 'hour': 244, 'doorstop': 245, 'style': 246, 'scottie': 247, 'writing': 248, 'cover': 249, 'rain': 250, 'pannetone': 251, 'cottage': 252, 'skipping': 253, 'rope': 254, 'cigar': 255, 'flower': 256, 'friends': 257, 'sugar': 258, 'purse': 259, 'lid': 260, 'breakfast': 261, 'apron': 262, 'smart': 263, 'fire': 264, 'plus': 265, 'chinese': 266, 'shirts': 267, 'for': 268, 'sponge': 269, 'spotty': 270, 'picture': 271, 'purple': 272, 'bracelet': 273, 'dove': 274, 'apple': 275, 'balloons': 276, 'basket': 277, 'trinket': 278, 'craft': 279, 'monitor': 280, 'tags': 281, 'tag': 282, 'save': 283, 'planet': 284, 'pegs': 285, 'harmonica': 286, 'dinosaur': 287, 'am': 288, 'teatime': 289, 'chick': 290, '36': 291, 'tidy': 292, 'childrens': 293, 'hate': 294, 'angels': 295, 'hldr': 296, 'toy': 297, 'finepix': 298, 'portable': 299, 'table': 300, 'scales': 301, 'chevrolet': 302, '100': 303, 'daisy': 304, 'natural': 305, 'time': 306, 'flag': 307, 'font': 308, 'w': 309, 'your': 310, 'own': 311, 'charm': 312, 'jug': 313, 'trousers': 314, 'pants': 315, 'alexa': 316, 'seaside': 317, 'rack': 318, 'knitted': 319, 'orange': 320, 'full': 321, 'oval': 322, 'diamante': 323, 'gift': 324, 'flying': 325, 'ducks': 326, 'cutters': 327, 'birthday': 328, 'charlie': 329, 'to': 330, 'invites': 331, 'rustic': 332, 'cushion': 333, 'candlestick': 334, 'mugs': 335, 'novel': 336, 'piano': 337, 'stripes': 338, 'clear': 339, 'baroquecandlestick': 340, 'keepsake': 341, 'fruits': 342, 'dominoes': 343, 'office': 344, 'lavender': 345, 'incense': 346, 'silk': 347, 'jar': 348, 'yellow': 349, 'saucer': 350, 'cutter': 351, \"children's\": 352, 'cherry': 353, 'inch': 354, 'sandwich': 355, 'plates': 356, 'funky': 357, 'classical': 358, 'snowy': 359, 'village': 360, 'decoupage': 361, 'rex': 362, 'cash': 363, 'carry': 364, 'pears': 365, 'game': 366, 'sticker': 367, 'picnic': 368, '4k': 369, 'model': 370, 'house': 371, 'little': 372, 'victorian': 373, '24': 374, 'jazz': 375, 'les': 376, 'paul': 377, 'electric': 378, 'rise': 379, 'wired': 380, 'usb': 381, 'mouse': 382, 'strand': 383, 'necklace': 384, 'crystal': 385, 'santa': 386, 'cooking': 387, 'sport': 388, 'wc': 389, 'wire': 390, 'amplifier': 391, 'first': 392, 'aid': 393, 'reindeer': 394, 'potting': 395, 'shed': 396, 'deluxe': 397, 'chilli': 398, '200': 399, '38': 400, 'container': 401, 'coloured': 402, 'psr': 403, 'keyboard': 404, 'family': 405, 'airline': 406, 'gin': 407, 'tonic': 408, 'diet': 409, 'plant': 410, 'angel': 411, 'rechargeable': 412, 'aa': 413, 'fancy': 414, 'organiser': 415, 'piggy': 416, 'top': 417, 'jewelled': 418, 'make': 419, \"you're\": 420, 'confusing': 421, 'aged': 422, 'pinks': 423, 'loose': 424, '0': 425, 'works': 426, 'hyacinth': 427, 'bulb': 428, 'grinder': 429, 'brown': 430, 'solid': 431, 'toadstools': 432, 'kings': 433, 'choice': 434, 'lovebird': 435, 'football': 436, 'bread': 437, 'bin': 438, 'felt': 439, 'baskets': 440, 'lids': 441, 'regency': 442, 'cakestand': 443, 'mirror': 444, 'blocks': 445, 'ring': 446, 'classic': 447, 'milk': 448, 'owls': 449, 'garland': 450, 'lace': 451, 'v': 452, 'neck': 453, 'roku': 454, 'hair': 455, 'grip': 456, 'campus': 457, 'sutra': 458, 'ball': 459, 'record': 460, 'sock': 461, 'cat': 462, 'stripey': 463, 'mice': 464, 'scissor': 465, 'teaspoons': 466, 'of4': 467, 'custom': 468, 'board': 469, 'magnets': 470, 'hyundai': 471, 'soap': 472, 'speaker': 473, 'girls': 474, 'postage': 475, 'erasers': 476, 'radio': 477, 'calculator': 478, 'eos': 479, 'grow': 480, '1920': 481, 'x': 482, '1080': 483, 'ips': 484, 'hdmi': 485, 'vga': 486, 'standard': 487, 'checked': 488, 'hat': 489, 'storage': 490, 'classroom': 491, 'elite': 492, 'print': 493, 'made': 494, 'trellis': 495, 'is': 496, 'stickers': 497, 'poncho': 498, 'dinner': 499, 'it': 500, 'square': 501, 'tissue': 502, 'toilet': 503, '500': 504, '40': 505, 'over': 506, '600': 507, 'snap': 508, 'cards': 509, 'scarf': 510, '11': 511, 'pc': 512, 'insignia': 513, 'ns': 514, '32df310na19': 515, '32': 516, 'edition': 517, '4xl': 518, '5xl': 519, 'clothing': 520, 'linen': 521, 'rowell': 522, 'trumpet': 523, 'tapes': 524, 'vase': 525, 'stationery': 526, 'spot': 527, 'valentine': 528, 'hawaiian': 529, 'grass': 530, 'skirt': 531, 'fern': 532, 'string': 533, 'candelabra': 534, 'shaped': 535, 'catch': 536, 'panda': 537, 'bunnies': 538, 'sheet': 539, 'doughnut': 540, '32gb': 541, 'latest': 542, 'cars': 543, 'santas': 544, 'lg': 545, '24m47vq': 546, 'lit': 547, 'three': 548, 'canvas': 549, 's770': 550, 'animals': 551, 'sl400': 552, 'clothes': 553, 'zeb': 554, '1200': 555, 'dpi': 556, 'optical': 557, 'high': 558, 'precision': 559, 'bake': 560, 'a': 561, 'so': 562, 'poorly': 563, 'molly': 564, 'm90': 565, 'foil': 566, 'recycling': 567, 'mens': 568, 'cool': 569, 'suit': 570, 'sports': 571, 'wear': 572, 'jogging': 573, 'suits': 574, 'short': 575, 'sleeve': 576, 'outfit': 577, 'sets': 578, 'tall': 579, 'cinammon': 580, 'cook': 581, 'wine': 582, 'painted': 583, 'denon': 584, 'spinning': 585, 'acoustic': 586, 'parkwood': 587, 'w81': 588, 'op': 589, 'engraved': 590, 'seed': 591, 'envelopes': 592, 'soft': 593, 'pinkwhite': 594, '3800': 595, 'namaste': 596, 'swagat': 597, 'snowflake': 598, 'rim': 599, 'placemats': 600, 'mint': 601, 'malibu': 602, 'paint': 603, 'stand': 604, 'med': 605, 'pirate': 606, 'chest': 607, '48': 608, 'pen': 609, 'ew400': 610, 'transfer': 611, 'tattoos': 612, 'gloves': 613, 'world': 614, 'champion': 615, 'ladder': 616, 'cone': 617, 'shelf': 618, 'stars': 619, 'on': 620, 'dress': 621, \"mother's\": 622, 'spoon': 623, 'rest': 624, 'etched': 625, 'tealight': 626, '700': 627, 'mah': 628, 'charger': 629, 'bp4': 630, 'secateurs': 631, 'sweet': 632, 'clip': 633, 'line': 634, 'sl380': 635, '400': 636, '46': 637, 'lamp': 638, 'beakers': 639, 'vanilla': 640, 'scent': 641, 'olan': 642, '450w': 643, 'buffels': 644, 'monsoon': 645, 'tassle': 646, 'p505': 647, 'caravan': 648, 'urban': 649, 'soldiers': 650, 'shimmering': 651, 'harlan': 652, 'long': 653, 'playtime': 654, 'amazon': 655, 'basics': 656, 'microwave': 657, 'cu': 658, 'ft': 659, '700w': 660, 'letters': 661, 'puzzles': 662, 'oven': 663, 'glove': 664, 'tins': 665, 'jardin': 666, 'de': 667, 'provence': 668, 'romantic': 669, 'metro': 670, 'parx': 671, 'tapered': 672, 'celebration': 673, 'birdhouse': 674, 'chicken': 675, 'cx40': 676, 'guns': 677, 'roses': 678, 'tower': 679, 'god': 680, 'part': 681, \"poppy's\": 682, 'playhouse': 683, 'bright': 684, 'blues': 685, 'mushroom': 686, 'dodge': 687, 'viper': 688, 'dinosaurs': 689, '15': 690, 'baubles': 691, 'looking': 692, 'expandable': 693, 'blouse': 694, 'bat': 695, 'sleeves': 696, 'hollow': 697, 'out': 698, 'xs': 699, '8xl': 700, 'advent': 701, 'calendar': 702, 'sack': 703, 'bathroom': 704, 'leaves': 705, 'soundbar': 706, 'hdr': 707, 'streaming': 708, 'media': 709, 'player': 710, 'exceptional': 711, 'audio': 712, 'includes': 713, 'voice': 714, 'remote': 715, 'montana': 716, 'sweetie': 717, 'colourblocked': 718, 'kids': 719, '44': 720, 'nissan': 721, 'sentra': 722, 'disco': 723, 'des': 724, 'shoulder': 725, 'cowboy': 726, 'road': 727, '150': 728, '62': 729, 'bauble': 730, 'ruby': 731, 'personal': 732, 'doorsign': 733, '433': 734, 'flowers': 735, 'magnet': 736, \"mum's\": 737, 'triple': 738, 'music': 739, 'man': 740, 'stingray': 741, 'eq': 742, 'h': 743, 'american': 744, 'bass': 745, 'snakes': 746, 'ladders': 747, 'go': 748, 'fair': 749, 'recipe': 750, 'bendy': 751, 'straws': 752, 'queens': 753, 'guard': 754, 'seventeen': 755, 'sideboard': 756, 'stick': 757, 'up': 758, 'cam': 759, 'security': 760, 'camera': 761, 'privacy': 762, 'controls': 763, 'simple': 764, 'setup': 765, 'backpack': 766, 'gazebo': 767, 'cupcake': 768, 'rabbit': 769, 'spykar': 770, 'adora': 771, 'skinny': 772, 'mid': 773, 'low': 774, 'distress': 775, 'ankle': 776, 'length': 777, 'stretchable': 778, 'jeans': 779, 'a300': 780, 'fridge': 781, 'us': 782, 'accent': 783, 'tale': 784, 'nightlight': 785, '350': 786, 'mirrored': 787, 'art': 788, 'gents': 789, 'boom': 790, 'medina': 791, 'rainy': 792, 'ladies': 793, 'alphabet': 794, 'pencil': 795, 'panasonic': 796, 'fm': 797, 'operated': 798, 'analog': 799, 'ac': 800, 'powered': 801, 'rf': 802, '2400d': 803, 'elegant': 804, 'tote': 805, 'cups': 806, 'cavalier': 807, 'candleholder': 808, '20da': 809, 'flannel': 810, 'doilies': 811, 'd30': 812, 'sow': 813, \"'n'\": 814, 'ear': 815, 'muff': 816, 'headphones': 817, 'trend': 818, 'letter': 819, 'deep': 820, 'sleeveless': 821, 'tank': 822, 'bell': 823, 'pro70': 824, 'acer': 825, 'sb220q': 826, 'bi': 827, '21': 828, 'inches': 829, 'ultra': 830, 'thin': 831, 'zero': 832, 'port': 833, 'olivia': 834, 'birdcage': 835, 'single': 836, 'rocking': 837, 'horse': 838, 'hi': 839, 'tec': 840, 'alpine': 841, 'z750': 842, 'malcom': 843, 'mendis': 844, 'decrotivevintage': 845, 'watering': 846, 'can': 847, 'marker': 848, 'bowls': 849, 'colorplus': 850, 'formal': 851, 'dressing': 852, 'gentleman': 853, 'repair': 854, 'basil': 855, 'ethnovogue': 856, 'beige': 857, 'measure': 858, 'kurta': 859, 'jacket': 860, 'wrap': 861, 'cowboys': 862, 'elantra': 863, 'wreath': 864, 'embroidered': 865, 'quilt': 866, 'all': 867, 'echo': 868, '4th': 869, 'gen': 870, 'glacier': 871, 'cones': 872, 'carnival': 873, 'f10': 874, 'zoom': 875, 'crazy': 876, 'supreme': 877, 'lark': 878, 'violin': 879, 'butterfly': 880, 'bunting': 881, 'clayderman': 882, 'peterson': 883, 'center': 884, 'eneloop': 885, 'market': 886, 'crates': 887, 'sx100': 888, '3d': 889, 'stamps': 890, 'violet': 891, 'evergarden': 892, 'faceted': 893, 'bangle': 894, 'turquoise': 895, 'boys': 896, 'flock': 897, 'pots': 898, 'plaque': 899, 'caddy': 900, 'ginger': 901, 'by': 902, 'lifestyle': 903, 'bowl': 904, 'chasing': 905, 'toys': 906, 'toyota': 907, 'tacoma': 908, 'instax': 909, 'instant': 910, 'film': 911, 'sheets': 912, '8': 913, 'cameras': 914, '4332059078': 915, 'sleeping': 916, 'cakes': 917, 'pattern': 918, '40i': 919, 'topiary': 920, 'saxophone': 921, 'soprano': 922, 'apples': 923, 'hairband': 924, 'fawn': 925, 'rosie': 926, 'birdy': 927, 'tape': 928, 'longboard': 929, 'ironing': 930, \"men's\": 931, 'athletic': 932, 'shorts': 933, 'jet': 934, 'afghan': 935, 'pair': 936, 'pens': 937, 'funny': 938, 'face': 939, 'dell': 940, '27': 941, 'backlit': 942, 'lcd': 943, 'se2719h': 944, '1080p': 945, 'at': 946, 'hz': 947, 'chrysanthemum': 948, 'epson': 949, 'photopc': 950, 'l': 951, '500v': 952, 'heads': 953, 'tails': 954, 'gold': 955, 'tray': 956, 'botanical': 957, 'five': 958, 'her': 959, 'served': 960, 'colouring': 961, 'rectangle': 962, 'chalkboard': 963, 'cable': 964, 'af': 965, 'extension': 966, 'computer': 967, 'spaceman': 968, 's500': 969, 'england': 970, 'hairbands': 971, 'saturn': 972, 'sl': 973, 'year': 974, 'mitsubishi': 975, 'mirage': 976, 'kensington': 977, 'take': 978, 'or': 979, 'leave': 980, 'present': 981, 'coasters': 982, 'hypernation': 983, 'open': 984, 'front': 985, 'shrug': 986, 'v7': 987}\n"
          ]
        }
      ],
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d3fe4d",
      "metadata": {
        "id": "47d3fe4d"
      },
      "outputs": [],
      "source": [
        "# Generate and pad the training sequences\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "maxlen = max([len(x) for x in sequences])\n",
        "padded = pad_sequences(sequences, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e72cdd5",
      "metadata": {
        "id": "9e72cdd5"
      },
      "outputs": [],
      "source": [
        "# Generate and pad the test sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d69c1606",
      "metadata": {
        "id": "d69c1606"
      },
      "outputs": [],
      "source": [
        "# Convert training labels to numeric\n",
        "y_train_final = []\n",
        "for i in Y_train:\n",
        "    if i == 'Dapur':\n",
        "        y_train_final.append(0)\n",
        "    elif i == 'Cafe':\n",
        "        y_train_final.append(1)\n",
        "    elif i == 'Hobi':\n",
        "        y_train_final.append(2)\n",
        "    elif i == 'Elektronik':\n",
        "        y_train_final.append(3)\n",
        "    elif i == 'Travel':\n",
        "        y_train_final.append(4)\n",
        "    elif i == 'Baju':\n",
        "        y_train_final.append(5)\n",
        "    elif i == 'Musik':\n",
        "        y_train_final.append(6)\n",
        "    elif i == 'Transportasi':\n",
        "        y_train_final.append(7)\n",
        "    elif i == 'Kamera':\n",
        "        y_train_final.append(8)\n",
        "y_train_final = np.array(y_train_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f3196d",
      "metadata": {
        "id": "49f3196d"
      },
      "outputs": [],
      "source": [
        "# Convert testing labels to numeric\n",
        "y_test_final = []\n",
        "for i in Y_test:\n",
        "    if i == 'Dapur':\n",
        "        y_test_final.append(0)\n",
        "    elif i == 'Cafe':\n",
        "        y_test_final.append(1)\n",
        "    elif i == 'Hobi':\n",
        "        y_test_final.append(2)\n",
        "    elif i == 'Elektronik':\n",
        "        y_test_final.append(3)\n",
        "    elif i == 'Travel':\n",
        "        y_test_final.append(4)\n",
        "    elif i == 'Baju':\n",
        "        y_test_final.append(5)\n",
        "    elif i == 'Musik':\n",
        "        y_test_final.append(6)\n",
        "    elif i == 'Transportasi':\n",
        "        y_test_final.append(7)\n",
        "    elif i == 'Kamera':\n",
        "        y_test_final.append(8)\n",
        "y_test_final = np.array(y_test_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9430e071",
      "metadata": {
        "id": "9430e071",
        "outputId": "64c12470-c67e-40de-8b69-9517812ef275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 21, 100)           98800     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 19, 32)            9632      \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               16640     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               33280     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 4617      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 425,625\n",
            "Trainable params: 425,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=maxlen),\n",
        "    tf.keras.layers.Conv1D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics='accuracy')\n",
        "\n",
        "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
        "                                     min_lr = 0.01,\n",
        "                                     monitor = 'val_loss',\n",
        "                                     verbose = 1)\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(padded, y_train_final, validation_data=(test_padded, y_test_final), epochs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mztQ0DKSPzz",
        "outputId": "2db5de7c-f27a-4616-902d-f34ec9677c5d"
      },
      "id": "9mztQ0DKSPzz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2106 - accuracy: 0.8903 - val_loss: 5.0494 - val_accuracy: 0.6306\n",
            "Epoch 2/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2082 - accuracy: 0.8903 - val_loss: 5.2290 - val_accuracy: 0.6261\n",
            "Epoch 3/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2087 - accuracy: 0.8846 - val_loss: 5.1458 - val_accuracy: 0.6171\n",
            "Epoch 4/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2063 - accuracy: 0.8869 - val_loss: 4.9085 - val_accuracy: 0.6171\n",
            "Epoch 5/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2119 - accuracy: 0.8778 - val_loss: 4.9824 - val_accuracy: 0.6216\n",
            "Epoch 6/1000\n",
            "28/28 [==============================] - 3s 108ms/step - loss: 0.2119 - accuracy: 0.8903 - val_loss: 5.0667 - val_accuracy: 0.6441\n",
            "Epoch 7/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2051 - accuracy: 0.8846 - val_loss: 5.1546 - val_accuracy: 0.6351\n",
            "Epoch 8/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2093 - accuracy: 0.8824 - val_loss: 4.9953 - val_accuracy: 0.6351\n",
            "Epoch 9/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2085 - accuracy: 0.8869 - val_loss: 5.0605 - val_accuracy: 0.6306\n",
            "Epoch 10/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2032 - accuracy: 0.8869 - val_loss: 5.0743 - val_accuracy: 0.6351\n",
            "Epoch 11/1000\n",
            "28/28 [==============================] - 2s 62ms/step - loss: 0.2090 - accuracy: 0.8835 - val_loss: 5.4094 - val_accuracy: 0.6306\n",
            "Epoch 12/1000\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.2065 - accuracy: 0.8869 - val_loss: 5.3343 - val_accuracy: 0.6351\n",
            "Epoch 13/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2052 - accuracy: 0.8835 - val_loss: 5.4099 - val_accuracy: 0.6126\n",
            "Epoch 14/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2057 - accuracy: 0.8869 - val_loss: 5.3686 - val_accuracy: 0.6126\n",
            "Epoch 15/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2051 - accuracy: 0.8891 - val_loss: 5.2090 - val_accuracy: 0.6261\n",
            "Epoch 16/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2086 - accuracy: 0.8869 - val_loss: 5.0009 - val_accuracy: 0.6306\n",
            "Epoch 17/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2065 - accuracy: 0.8824 - val_loss: 5.1689 - val_accuracy: 0.6351\n",
            "Epoch 18/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2105 - accuracy: 0.8846 - val_loss: 5.4298 - val_accuracy: 0.6396\n",
            "Epoch 19/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2051 - accuracy: 0.8801 - val_loss: 5.5993 - val_accuracy: 0.6396\n",
            "Epoch 20/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2092 - accuracy: 0.8801 - val_loss: 5.4636 - val_accuracy: 0.6171\n",
            "Epoch 21/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2073 - accuracy: 0.8891 - val_loss: 5.1820 - val_accuracy: 0.6351\n",
            "Epoch 22/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2159 - accuracy: 0.8914 - val_loss: 5.2850 - val_accuracy: 0.6351\n",
            "Epoch 23/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2027 - accuracy: 0.8891 - val_loss: 5.3888 - val_accuracy: 0.6396\n",
            "Epoch 24/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2039 - accuracy: 0.8903 - val_loss: 5.4955 - val_accuracy: 0.6441\n",
            "Epoch 25/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2052 - accuracy: 0.8857 - val_loss: 5.3312 - val_accuracy: 0.6261\n",
            "Epoch 26/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2144 - accuracy: 0.8835 - val_loss: 5.3447 - val_accuracy: 0.6306\n",
            "Epoch 27/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2140 - accuracy: 0.8778 - val_loss: 5.1543 - val_accuracy: 0.6396\n",
            "Epoch 28/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2056 - accuracy: 0.8880 - val_loss: 5.4049 - val_accuracy: 0.6081\n",
            "Epoch 29/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2083 - accuracy: 0.8846 - val_loss: 5.7035 - val_accuracy: 0.6036\n",
            "Epoch 30/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2078 - accuracy: 0.8824 - val_loss: 5.4552 - val_accuracy: 0.5946\n",
            "Epoch 31/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2073 - accuracy: 0.8835 - val_loss: 5.4729 - val_accuracy: 0.6081\n",
            "Epoch 32/1000\n",
            "28/28 [==============================] - 3s 114ms/step - loss: 0.2030 - accuracy: 0.8857 - val_loss: 5.8206 - val_accuracy: 0.6036\n",
            "Epoch 33/1000\n",
            "28/28 [==============================] - 3s 118ms/step - loss: 0.2180 - accuracy: 0.8891 - val_loss: 6.1006 - val_accuracy: 0.5946\n",
            "Epoch 34/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2153 - accuracy: 0.8857 - val_loss: 5.5865 - val_accuracy: 0.6216\n",
            "Epoch 35/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2058 - accuracy: 0.8880 - val_loss: 5.4951 - val_accuracy: 0.6171\n",
            "Epoch 36/1000\n",
            "28/28 [==============================] - 2s 62ms/step - loss: 0.2087 - accuracy: 0.8812 - val_loss: 5.8354 - val_accuracy: 0.6036\n",
            "Epoch 37/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2059 - accuracy: 0.8824 - val_loss: 5.5787 - val_accuracy: 0.5946\n",
            "Epoch 38/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2066 - accuracy: 0.8835 - val_loss: 5.6476 - val_accuracy: 0.6216\n",
            "Epoch 39/1000\n",
            "28/28 [==============================] - 2s 90ms/step - loss: 0.2113 - accuracy: 0.8857 - val_loss: 5.5090 - val_accuracy: 0.6081\n",
            "Epoch 40/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2105 - accuracy: 0.8835 - val_loss: 5.3796 - val_accuracy: 0.6306\n",
            "Epoch 41/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2078 - accuracy: 0.8857 - val_loss: 5.6258 - val_accuracy: 0.6441\n",
            "Epoch 42/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2070 - accuracy: 0.8880 - val_loss: 5.7285 - val_accuracy: 0.6306\n",
            "Epoch 43/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2108 - accuracy: 0.8778 - val_loss: 5.4268 - val_accuracy: 0.6171\n",
            "Epoch 44/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2065 - accuracy: 0.8903 - val_loss: 5.3971 - val_accuracy: 0.6441\n",
            "Epoch 45/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2055 - accuracy: 0.8869 - val_loss: 5.4076 - val_accuracy: 0.6306\n",
            "Epoch 46/1000\n",
            "28/28 [==============================] - 3s 110ms/step - loss: 0.2039 - accuracy: 0.8824 - val_loss: 5.5808 - val_accuracy: 0.6171\n",
            "Epoch 47/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2077 - accuracy: 0.8903 - val_loss: 5.5028 - val_accuracy: 0.6036\n",
            "Epoch 48/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2060 - accuracy: 0.8812 - val_loss: 5.8924 - val_accuracy: 0.6126\n",
            "Epoch 49/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2173 - accuracy: 0.8835 - val_loss: 5.3517 - val_accuracy: 0.6306\n",
            "Epoch 50/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2083 - accuracy: 0.8891 - val_loss: 5.3736 - val_accuracy: 0.6171\n",
            "Epoch 51/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2100 - accuracy: 0.8869 - val_loss: 5.2764 - val_accuracy: 0.6081\n",
            "Epoch 52/1000\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.2122 - accuracy: 0.8801 - val_loss: 5.4010 - val_accuracy: 0.6081\n",
            "Epoch 53/1000\n",
            "28/28 [==============================] - 3s 89ms/step - loss: 0.2025 - accuracy: 0.8880 - val_loss: 5.5942 - val_accuracy: 0.6036\n",
            "Epoch 54/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2113 - accuracy: 0.8937 - val_loss: 5.6330 - val_accuracy: 0.5991\n",
            "Epoch 55/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2054 - accuracy: 0.8857 - val_loss: 5.5157 - val_accuracy: 0.6126\n",
            "Epoch 56/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2067 - accuracy: 0.8857 - val_loss: 5.2585 - val_accuracy: 0.6036\n",
            "Epoch 57/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2326 - accuracy: 0.8846 - val_loss: 5.2403 - val_accuracy: 0.6126\n",
            "Epoch 58/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2045 - accuracy: 0.8880 - val_loss: 5.3707 - val_accuracy: 0.6171\n",
            "Epoch 59/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2049 - accuracy: 0.8790 - val_loss: 5.4650 - val_accuracy: 0.6081\n",
            "Epoch 60/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2050 - accuracy: 0.8801 - val_loss: 5.9325 - val_accuracy: 0.6216\n",
            "Epoch 61/1000\n",
            "28/28 [==============================] - 2s 62ms/step - loss: 0.2158 - accuracy: 0.8846 - val_loss: 6.5291 - val_accuracy: 0.5946\n",
            "Epoch 62/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2061 - accuracy: 0.8880 - val_loss: 5.9979 - val_accuracy: 0.6036\n",
            "Epoch 63/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2053 - accuracy: 0.8948 - val_loss: 5.9470 - val_accuracy: 0.5946\n",
            "Epoch 64/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2083 - accuracy: 0.8880 - val_loss: 5.9318 - val_accuracy: 0.6126\n",
            "Epoch 65/1000\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.2093 - accuracy: 0.8891 - val_loss: 5.5132 - val_accuracy: 0.6171\n",
            "Epoch 66/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.2125 - accuracy: 0.8767 - val_loss: 5.6982 - val_accuracy: 0.6081\n",
            "Epoch 67/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2053 - accuracy: 0.8824 - val_loss: 5.6374 - val_accuracy: 0.6126\n",
            "Epoch 68/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2063 - accuracy: 0.8846 - val_loss: 6.0775 - val_accuracy: 0.6036\n",
            "Epoch 69/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2053 - accuracy: 0.8790 - val_loss: 6.0686 - val_accuracy: 0.6081\n",
            "Epoch 70/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2071 - accuracy: 0.8869 - val_loss: 5.9752 - val_accuracy: 0.6036\n",
            "Epoch 71/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2103 - accuracy: 0.8778 - val_loss: 5.7425 - val_accuracy: 0.6171\n",
            "Epoch 72/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2034 - accuracy: 0.8869 - val_loss: 5.7126 - val_accuracy: 0.6216\n",
            "Epoch 73/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2091 - accuracy: 0.8846 - val_loss: 5.6150 - val_accuracy: 0.5991\n",
            "Epoch 74/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2061 - accuracy: 0.8914 - val_loss: 5.3397 - val_accuracy: 0.5991\n",
            "Epoch 75/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2066 - accuracy: 0.8925 - val_loss: 5.5897 - val_accuracy: 0.6216\n",
            "Epoch 76/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2107 - accuracy: 0.8869 - val_loss: 5.6797 - val_accuracy: 0.6126\n",
            "Epoch 77/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2180 - accuracy: 0.8812 - val_loss: 5.7320 - val_accuracy: 0.6081\n",
            "Epoch 78/1000\n",
            "28/28 [==============================] - 2s 79ms/step - loss: 0.2083 - accuracy: 0.8778 - val_loss: 5.7860 - val_accuracy: 0.5946\n",
            "Epoch 79/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2049 - accuracy: 0.8869 - val_loss: 5.8499 - val_accuracy: 0.5901\n",
            "Epoch 80/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2014 - accuracy: 0.8903 - val_loss: 5.9577 - val_accuracy: 0.5901\n",
            "Epoch 81/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2055 - accuracy: 0.8812 - val_loss: 6.0718 - val_accuracy: 0.5901\n",
            "Epoch 82/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2074 - accuracy: 0.8869 - val_loss: 6.1053 - val_accuracy: 0.6036\n",
            "Epoch 83/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2035 - accuracy: 0.8903 - val_loss: 6.1211 - val_accuracy: 0.6036\n",
            "Epoch 84/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2118 - accuracy: 0.8835 - val_loss: 6.0136 - val_accuracy: 0.6081\n",
            "Epoch 85/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.2024 - accuracy: 0.8903 - val_loss: 6.1835 - val_accuracy: 0.6036\n",
            "Epoch 86/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2092 - accuracy: 0.8824 - val_loss: 5.8392 - val_accuracy: 0.5901\n",
            "Epoch 87/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2102 - accuracy: 0.8869 - val_loss: 5.9148 - val_accuracy: 0.6081\n",
            "Epoch 88/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2109 - accuracy: 0.8824 - val_loss: 6.1238 - val_accuracy: 0.5946\n",
            "Epoch 89/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2076 - accuracy: 0.8857 - val_loss: 5.8748 - val_accuracy: 0.6036\n",
            "Epoch 90/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2099 - accuracy: 0.8891 - val_loss: 6.0139 - val_accuracy: 0.5991\n",
            "Epoch 91/1000\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.2054 - accuracy: 0.8801 - val_loss: 6.3240 - val_accuracy: 0.6126\n",
            "Epoch 92/1000\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.2038 - accuracy: 0.8869 - val_loss: 6.1945 - val_accuracy: 0.5901\n",
            "Epoch 93/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2109 - accuracy: 0.8846 - val_loss: 6.1003 - val_accuracy: 0.6081\n",
            "Epoch 94/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2083 - accuracy: 0.8846 - val_loss: 5.8224 - val_accuracy: 0.6081\n",
            "Epoch 95/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2070 - accuracy: 0.8869 - val_loss: 5.9960 - val_accuracy: 0.5901\n",
            "Epoch 96/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2129 - accuracy: 0.8857 - val_loss: 6.0112 - val_accuracy: 0.5946\n",
            "Epoch 97/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2104 - accuracy: 0.8914 - val_loss: 6.0095 - val_accuracy: 0.5946\n",
            "Epoch 98/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2081 - accuracy: 0.8869 - val_loss: 6.3130 - val_accuracy: 0.6036\n",
            "Epoch 99/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2048 - accuracy: 0.8880 - val_loss: 6.1595 - val_accuracy: 0.6081\n",
            "Epoch 100/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2100 - accuracy: 0.8857 - val_loss: 5.8850 - val_accuracy: 0.6036\n",
            "Epoch 101/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2014 - accuracy: 0.8880 - val_loss: 6.5090 - val_accuracy: 0.5991\n",
            "Epoch 102/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2030 - accuracy: 0.8937 - val_loss: 6.9339 - val_accuracy: 0.5991\n",
            "Epoch 103/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2157 - accuracy: 0.8778 - val_loss: 5.4101 - val_accuracy: 0.6036\n",
            "Epoch 104/1000\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.2079 - accuracy: 0.8869 - val_loss: 6.0744 - val_accuracy: 0.5811\n",
            "Epoch 105/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2091 - accuracy: 0.8880 - val_loss: 6.3428 - val_accuracy: 0.6081\n",
            "Epoch 106/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2034 - accuracy: 0.8812 - val_loss: 6.1038 - val_accuracy: 0.5991\n",
            "Epoch 107/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2009 - accuracy: 0.8880 - val_loss: 6.3540 - val_accuracy: 0.6036\n",
            "Epoch 108/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2076 - accuracy: 0.8846 - val_loss: 6.3851 - val_accuracy: 0.6081\n",
            "Epoch 109/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2079 - accuracy: 0.8733 - val_loss: 6.5523 - val_accuracy: 0.6036\n",
            "Epoch 110/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2035 - accuracy: 0.8835 - val_loss: 6.3926 - val_accuracy: 0.6081\n",
            "Epoch 111/1000\n",
            "28/28 [==============================] - 3s 100ms/step - loss: 0.2176 - accuracy: 0.8778 - val_loss: 6.0110 - val_accuracy: 0.6036\n",
            "Epoch 112/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2049 - accuracy: 0.8914 - val_loss: 5.7825 - val_accuracy: 0.5991\n",
            "Epoch 113/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2036 - accuracy: 0.8756 - val_loss: 5.8769 - val_accuracy: 0.6081\n",
            "Epoch 114/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2054 - accuracy: 0.8914 - val_loss: 5.9788 - val_accuracy: 0.6126\n",
            "Epoch 115/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2055 - accuracy: 0.8846 - val_loss: 5.9973 - val_accuracy: 0.6216\n",
            "Epoch 116/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2050 - accuracy: 0.8835 - val_loss: 5.9917 - val_accuracy: 0.6081\n",
            "Epoch 117/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2046 - accuracy: 0.8959 - val_loss: 6.1148 - val_accuracy: 0.6081\n",
            "Epoch 118/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2107 - accuracy: 0.8835 - val_loss: 5.8454 - val_accuracy: 0.6171\n",
            "Epoch 119/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2044 - accuracy: 0.8914 - val_loss: 6.0839 - val_accuracy: 0.6081\n",
            "Epoch 120/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2065 - accuracy: 0.8801 - val_loss: 6.3499 - val_accuracy: 0.6081\n",
            "Epoch 121/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2027 - accuracy: 0.8869 - val_loss: 6.7186 - val_accuracy: 0.5946\n",
            "Epoch 122/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2023 - accuracy: 0.8869 - val_loss: 6.6321 - val_accuracy: 0.5991\n",
            "Epoch 123/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2054 - accuracy: 0.8835 - val_loss: 6.6864 - val_accuracy: 0.5946\n",
            "Epoch 124/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2048 - accuracy: 0.8846 - val_loss: 6.6018 - val_accuracy: 0.5901\n",
            "Epoch 125/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2062 - accuracy: 0.8869 - val_loss: 6.8783 - val_accuracy: 0.6081\n",
            "Epoch 126/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2046 - accuracy: 0.8903 - val_loss: 6.4747 - val_accuracy: 0.5991\n",
            "Epoch 127/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2049 - accuracy: 0.8903 - val_loss: 6.3168 - val_accuracy: 0.6126\n",
            "Epoch 128/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2023 - accuracy: 0.8948 - val_loss: 6.2269 - val_accuracy: 0.6171\n",
            "Epoch 129/1000\n",
            "28/28 [==============================] - 3s 100ms/step - loss: 0.2037 - accuracy: 0.8903 - val_loss: 6.2665 - val_accuracy: 0.6081\n",
            "Epoch 130/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2046 - accuracy: 0.8835 - val_loss: 6.4074 - val_accuracy: 0.5901\n",
            "Epoch 131/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2093 - accuracy: 0.8824 - val_loss: 6.3575 - val_accuracy: 0.5991\n",
            "Epoch 132/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2047 - accuracy: 0.8903 - val_loss: 6.5662 - val_accuracy: 0.5991\n",
            "Epoch 133/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2012 - accuracy: 0.8767 - val_loss: 6.6409 - val_accuracy: 0.6126\n",
            "Epoch 134/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2097 - accuracy: 0.8812 - val_loss: 6.3715 - val_accuracy: 0.6036\n",
            "Epoch 135/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2075 - accuracy: 0.8857 - val_loss: 6.1011 - val_accuracy: 0.6126\n",
            "Epoch 136/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2062 - accuracy: 0.8937 - val_loss: 6.0632 - val_accuracy: 0.6081\n",
            "Epoch 137/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2103 - accuracy: 0.8869 - val_loss: 5.4188 - val_accuracy: 0.5946\n",
            "Epoch 138/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2095 - accuracy: 0.8790 - val_loss: 5.4721 - val_accuracy: 0.5946\n",
            "Epoch 139/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2104 - accuracy: 0.8812 - val_loss: 5.5122 - val_accuracy: 0.6261\n",
            "Epoch 140/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2058 - accuracy: 0.8857 - val_loss: 5.7235 - val_accuracy: 0.6351\n",
            "Epoch 141/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2129 - accuracy: 0.8744 - val_loss: 5.6548 - val_accuracy: 0.6351\n",
            "Epoch 142/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2030 - accuracy: 0.8857 - val_loss: 5.7964 - val_accuracy: 0.6216\n",
            "Epoch 143/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2046 - accuracy: 0.8869 - val_loss: 5.9312 - val_accuracy: 0.6216\n",
            "Epoch 144/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2070 - accuracy: 0.8925 - val_loss: 6.1556 - val_accuracy: 0.5946\n",
            "Epoch 145/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2138 - accuracy: 0.8790 - val_loss: 5.3911 - val_accuracy: 0.5901\n",
            "Epoch 146/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2106 - accuracy: 0.8801 - val_loss: 5.7956 - val_accuracy: 0.5991\n",
            "Epoch 147/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2034 - accuracy: 0.8869 - val_loss: 5.9595 - val_accuracy: 0.5856\n",
            "Epoch 148/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2062 - accuracy: 0.8835 - val_loss: 6.2481 - val_accuracy: 0.6036\n",
            "Epoch 149/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2098 - accuracy: 0.8869 - val_loss: 5.8245 - val_accuracy: 0.5991\n",
            "Epoch 150/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2057 - accuracy: 0.8891 - val_loss: 6.2285 - val_accuracy: 0.5991\n",
            "Epoch 151/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2098 - accuracy: 0.8790 - val_loss: 6.7587 - val_accuracy: 0.6081\n",
            "Epoch 152/1000\n",
            "28/28 [==============================] - 2s 62ms/step - loss: 0.2089 - accuracy: 0.8880 - val_loss: 6.4560 - val_accuracy: 0.5766\n",
            "Epoch 153/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2095 - accuracy: 0.8869 - val_loss: 6.4546 - val_accuracy: 0.6081\n",
            "Epoch 154/1000\n",
            "28/28 [==============================] - 2s 62ms/step - loss: 0.2124 - accuracy: 0.8891 - val_loss: 6.3872 - val_accuracy: 0.6171\n",
            "Epoch 155/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2091 - accuracy: 0.8857 - val_loss: 6.1845 - val_accuracy: 0.6036\n",
            "Epoch 156/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2065 - accuracy: 0.8835 - val_loss: 6.3098 - val_accuracy: 0.6261\n",
            "Epoch 157/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2048 - accuracy: 0.8959 - val_loss: 6.5454 - val_accuracy: 0.6306\n",
            "Epoch 158/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2166 - accuracy: 0.8824 - val_loss: 5.8996 - val_accuracy: 0.5991\n",
            "Epoch 159/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2131 - accuracy: 0.8801 - val_loss: 5.5825 - val_accuracy: 0.5946\n",
            "Epoch 160/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2063 - accuracy: 0.8914 - val_loss: 6.0369 - val_accuracy: 0.6081\n",
            "Epoch 161/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2036 - accuracy: 0.8925 - val_loss: 6.3239 - val_accuracy: 0.6171\n",
            "Epoch 162/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2008 - accuracy: 0.8903 - val_loss: 6.4434 - val_accuracy: 0.5991\n",
            "Epoch 163/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2034 - accuracy: 0.8857 - val_loss: 6.4565 - val_accuracy: 0.6081\n",
            "Epoch 164/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2051 - accuracy: 0.8824 - val_loss: 6.6331 - val_accuracy: 0.5991\n",
            "Epoch 165/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2050 - accuracy: 0.8846 - val_loss: 6.6862 - val_accuracy: 0.5901\n",
            "Epoch 166/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2054 - accuracy: 0.8903 - val_loss: 6.7125 - val_accuracy: 0.5856\n",
            "Epoch 167/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2063 - accuracy: 0.8824 - val_loss: 6.2157 - val_accuracy: 0.6171\n",
            "Epoch 168/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2089 - accuracy: 0.8948 - val_loss: 5.9953 - val_accuracy: 0.6216\n",
            "Epoch 169/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.2114 - accuracy: 0.8903 - val_loss: 6.4847 - val_accuracy: 0.6171\n",
            "Epoch 170/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2072 - accuracy: 0.8869 - val_loss: 6.3404 - val_accuracy: 0.5991\n",
            "Epoch 171/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2128 - accuracy: 0.8846 - val_loss: 6.2709 - val_accuracy: 0.6216\n",
            "Epoch 172/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2055 - accuracy: 0.8824 - val_loss: 5.9014 - val_accuracy: 0.5991\n",
            "Epoch 173/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2028 - accuracy: 0.8846 - val_loss: 6.1401 - val_accuracy: 0.6216\n",
            "Epoch 174/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2041 - accuracy: 0.8801 - val_loss: 6.6524 - val_accuracy: 0.6126\n",
            "Epoch 175/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2062 - accuracy: 0.8869 - val_loss: 6.8740 - val_accuracy: 0.6081\n",
            "Epoch 176/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2043 - accuracy: 0.8846 - val_loss: 6.7253 - val_accuracy: 0.6036\n",
            "Epoch 177/1000\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.2041 - accuracy: 0.8790 - val_loss: 6.6814 - val_accuracy: 0.6126\n",
            "Epoch 178/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2003 - accuracy: 0.8880 - val_loss: 6.6971 - val_accuracy: 0.6216\n",
            "Epoch 179/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2079 - accuracy: 0.8846 - val_loss: 6.7160 - val_accuracy: 0.6306\n",
            "Epoch 180/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2015 - accuracy: 0.8880 - val_loss: 6.9325 - val_accuracy: 0.6036\n",
            "Epoch 181/1000\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.2059 - accuracy: 0.8824 - val_loss: 6.6064 - val_accuracy: 0.6216\n",
            "Epoch 182/1000\n",
            "28/28 [==============================] - 2s 88ms/step - loss: 0.2065 - accuracy: 0.8891 - val_loss: 6.2698 - val_accuracy: 0.6171\n",
            "Epoch 183/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2024 - accuracy: 0.8846 - val_loss: 6.4634 - val_accuracy: 0.6171\n",
            "Epoch 184/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2000 - accuracy: 0.8880 - val_loss: 6.5501 - val_accuracy: 0.6216\n",
            "Epoch 185/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2036 - accuracy: 0.8790 - val_loss: 6.5222 - val_accuracy: 0.6306\n",
            "Epoch 186/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2022 - accuracy: 0.8880 - val_loss: 6.6388 - val_accuracy: 0.6261\n",
            "Epoch 187/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2070 - accuracy: 0.8869 - val_loss: 6.5325 - val_accuracy: 0.6216\n",
            "Epoch 188/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2081 - accuracy: 0.8835 - val_loss: 6.6360 - val_accuracy: 0.5946\n",
            "Epoch 189/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2051 - accuracy: 0.8857 - val_loss: 6.6093 - val_accuracy: 0.6036\n",
            "Epoch 190/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2046 - accuracy: 0.8880 - val_loss: 6.5324 - val_accuracy: 0.6036\n",
            "Epoch 191/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2060 - accuracy: 0.8857 - val_loss: 6.4430 - val_accuracy: 0.5901\n",
            "Epoch 192/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2034 - accuracy: 0.8880 - val_loss: 6.4207 - val_accuracy: 0.5946\n",
            "Epoch 193/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2026 - accuracy: 0.8812 - val_loss: 6.8573 - val_accuracy: 0.6261\n",
            "Epoch 194/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2044 - accuracy: 0.8903 - val_loss: 6.9962 - val_accuracy: 0.6261\n",
            "Epoch 195/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2036 - accuracy: 0.8835 - val_loss: 6.8469 - val_accuracy: 0.6171\n",
            "Epoch 196/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2041 - accuracy: 0.8880 - val_loss: 6.5491 - val_accuracy: 0.6081\n",
            "Epoch 197/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2044 - accuracy: 0.8812 - val_loss: 6.5558 - val_accuracy: 0.6171\n",
            "Epoch 198/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2080 - accuracy: 0.8835 - val_loss: 6.4852 - val_accuracy: 0.6261\n",
            "Epoch 199/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2019 - accuracy: 0.8914 - val_loss: 6.7569 - val_accuracy: 0.6216\n",
            "Epoch 200/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2051 - accuracy: 0.8925 - val_loss: 6.6373 - val_accuracy: 0.6171\n",
            "Epoch 201/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2103 - accuracy: 0.8857 - val_loss: 6.4733 - val_accuracy: 0.6261\n",
            "Epoch 202/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2036 - accuracy: 0.8790 - val_loss: 6.5742 - val_accuracy: 0.6171\n",
            "Epoch 203/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2079 - accuracy: 0.8801 - val_loss: 6.4472 - val_accuracy: 0.6171\n",
            "Epoch 204/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2126 - accuracy: 0.8914 - val_loss: 5.9113 - val_accuracy: 0.6216\n",
            "Epoch 205/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2037 - accuracy: 0.8857 - val_loss: 5.9873 - val_accuracy: 0.6171\n",
            "Epoch 206/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2094 - accuracy: 0.8914 - val_loss: 5.7667 - val_accuracy: 0.6171\n",
            "Epoch 207/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2021 - accuracy: 0.8937 - val_loss: 5.7180 - val_accuracy: 0.6171\n",
            "Epoch 208/1000\n",
            "28/28 [==============================] - 2s 88ms/step - loss: 0.2010 - accuracy: 0.8925 - val_loss: 6.5028 - val_accuracy: 0.6261\n",
            "Epoch 209/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2194 - accuracy: 0.8744 - val_loss: 5.7803 - val_accuracy: 0.5901\n",
            "Epoch 210/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2093 - accuracy: 0.8744 - val_loss: 5.7965 - val_accuracy: 0.5946\n",
            "Epoch 211/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2041 - accuracy: 0.8857 - val_loss: 5.8857 - val_accuracy: 0.6081\n",
            "Epoch 212/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2010 - accuracy: 0.8925 - val_loss: 6.2370 - val_accuracy: 0.6081\n",
            "Epoch 213/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2050 - accuracy: 0.8790 - val_loss: 6.4531 - val_accuracy: 0.6081\n",
            "Epoch 214/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2020 - accuracy: 0.8880 - val_loss: 6.5516 - val_accuracy: 0.6036\n",
            "Epoch 215/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2013 - accuracy: 0.8824 - val_loss: 6.4518 - val_accuracy: 0.5991\n",
            "Epoch 216/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2027 - accuracy: 0.8891 - val_loss: 6.5994 - val_accuracy: 0.5991\n",
            "Epoch 217/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2079 - accuracy: 0.8801 - val_loss: 6.5200 - val_accuracy: 0.6171\n",
            "Epoch 218/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1997 - accuracy: 0.8971 - val_loss: 6.7280 - val_accuracy: 0.6126\n",
            "Epoch 219/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2050 - accuracy: 0.8937 - val_loss: 6.7873 - val_accuracy: 0.6261\n",
            "Epoch 220/1000\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.2024 - accuracy: 0.8880 - val_loss: 6.5383 - val_accuracy: 0.6171\n",
            "Epoch 221/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2076 - accuracy: 0.8812 - val_loss: 6.6670 - val_accuracy: 0.6216\n",
            "Epoch 222/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2054 - accuracy: 0.8891 - val_loss: 6.2524 - val_accuracy: 0.6036\n",
            "Epoch 223/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2002 - accuracy: 0.8869 - val_loss: 6.3375 - val_accuracy: 0.6171\n",
            "Epoch 224/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2043 - accuracy: 0.8914 - val_loss: 6.5045 - val_accuracy: 0.6126\n",
            "Epoch 225/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2036 - accuracy: 0.8846 - val_loss: 6.5391 - val_accuracy: 0.6036\n",
            "Epoch 226/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2047 - accuracy: 0.8903 - val_loss: 6.6267 - val_accuracy: 0.6171\n",
            "Epoch 227/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.1987 - accuracy: 0.8993 - val_loss: 6.6413 - val_accuracy: 0.6081\n",
            "Epoch 228/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2018 - accuracy: 0.8801 - val_loss: 6.4948 - val_accuracy: 0.6216\n",
            "Epoch 229/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2064 - accuracy: 0.8846 - val_loss: 6.5079 - val_accuracy: 0.6261\n",
            "Epoch 230/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.1989 - accuracy: 0.8846 - val_loss: 6.5984 - val_accuracy: 0.6126\n",
            "Epoch 231/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2051 - accuracy: 0.8891 - val_loss: 6.5893 - val_accuracy: 0.5811\n",
            "Epoch 232/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.2112 - accuracy: 0.8801 - val_loss: 6.1856 - val_accuracy: 0.6171\n",
            "Epoch 233/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2003 - accuracy: 0.8869 - val_loss: 6.1812 - val_accuracy: 0.6216\n",
            "Epoch 234/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2046 - accuracy: 0.8857 - val_loss: 6.2717 - val_accuracy: 0.5946\n",
            "Epoch 235/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2039 - accuracy: 0.8801 - val_loss: 6.3822 - val_accuracy: 0.6216\n",
            "Epoch 236/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.1985 - accuracy: 0.8880 - val_loss: 6.4623 - val_accuracy: 0.6126\n",
            "Epoch 237/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2069 - accuracy: 0.8869 - val_loss: 6.4968 - val_accuracy: 0.6261\n",
            "Epoch 238/1000\n",
            "28/28 [==============================] - 2s 64ms/step - loss: 0.2055 - accuracy: 0.8812 - val_loss: 6.4366 - val_accuracy: 0.6216\n",
            "Epoch 239/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2068 - accuracy: 0.8778 - val_loss: 6.6253 - val_accuracy: 0.6081\n",
            "Epoch 240/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2185 - accuracy: 0.8880 - val_loss: 5.8857 - val_accuracy: 0.6261\n",
            "Epoch 241/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2246 - accuracy: 0.8790 - val_loss: 6.2989 - val_accuracy: 0.6171\n",
            "Epoch 242/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2038 - accuracy: 0.8778 - val_loss: 6.6315 - val_accuracy: 0.6036\n",
            "Epoch 243/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2120 - accuracy: 0.8824 - val_loss: 6.1113 - val_accuracy: 0.6081\n",
            "Epoch 244/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2101 - accuracy: 0.8891 - val_loss: 6.1758 - val_accuracy: 0.5991\n",
            "Epoch 245/1000\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.2059 - accuracy: 0.8880 - val_loss: 6.1927 - val_accuracy: 0.6126\n",
            "Epoch 246/1000\n",
            "28/28 [==============================] - 3s 89ms/step - loss: 0.2083 - accuracy: 0.8891 - val_loss: 6.2786 - val_accuracy: 0.6081\n",
            "Epoch 247/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2087 - accuracy: 0.8801 - val_loss: 6.4286 - val_accuracy: 0.5946\n",
            "Epoch 248/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2027 - accuracy: 0.8824 - val_loss: 6.4950 - val_accuracy: 0.6171\n",
            "Epoch 249/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2105 - accuracy: 0.8869 - val_loss: 6.1917 - val_accuracy: 0.6081\n",
            "Epoch 250/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2020 - accuracy: 0.8925 - val_loss: 6.2589 - val_accuracy: 0.6036\n",
            "Epoch 251/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2045 - accuracy: 0.8869 - val_loss: 6.4023 - val_accuracy: 0.6081\n",
            "Epoch 252/1000\n",
            "28/28 [==============================] - 3s 108ms/step - loss: 0.2053 - accuracy: 0.8925 - val_loss: 6.3206 - val_accuracy: 0.6081\n",
            "Epoch 253/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2046 - accuracy: 0.8925 - val_loss: 6.7930 - val_accuracy: 0.6081\n",
            "Epoch 254/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2100 - accuracy: 0.8869 - val_loss: 6.7396 - val_accuracy: 0.5991\n",
            "Epoch 255/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2157 - accuracy: 0.8835 - val_loss: 5.9713 - val_accuracy: 0.6081\n",
            "Epoch 256/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2019 - accuracy: 0.8824 - val_loss: 6.1573 - val_accuracy: 0.6081\n",
            "Epoch 257/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2031 - accuracy: 0.8880 - val_loss: 6.3699 - val_accuracy: 0.6036\n",
            "Epoch 258/1000\n",
            "28/28 [==============================] - 3s 100ms/step - loss: 0.2146 - accuracy: 0.8812 - val_loss: 5.8797 - val_accuracy: 0.5991\n",
            "Epoch 259/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2111 - accuracy: 0.8835 - val_loss: 5.7311 - val_accuracy: 0.5946\n",
            "Epoch 260/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2100 - accuracy: 0.8903 - val_loss: 5.9771 - val_accuracy: 0.6396\n",
            "Epoch 261/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2026 - accuracy: 0.8880 - val_loss: 6.1605 - val_accuracy: 0.6351\n",
            "Epoch 262/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2037 - accuracy: 0.8835 - val_loss: 6.3521 - val_accuracy: 0.6351\n",
            "Epoch 263/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2066 - accuracy: 0.8869 - val_loss: 6.0470 - val_accuracy: 0.6081\n",
            "Epoch 264/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.2109 - accuracy: 0.8824 - val_loss: 6.0237 - val_accuracy: 0.6036\n",
            "Epoch 265/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2047 - accuracy: 0.8857 - val_loss: 6.0067 - val_accuracy: 0.6351\n",
            "Epoch 266/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2048 - accuracy: 0.8869 - val_loss: 5.7492 - val_accuracy: 0.6126\n",
            "Epoch 267/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2069 - accuracy: 0.8824 - val_loss: 5.6873 - val_accuracy: 0.6171\n",
            "Epoch 268/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2045 - accuracy: 0.8891 - val_loss: 5.8539 - val_accuracy: 0.6171\n",
            "Epoch 269/1000\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.2053 - accuracy: 0.8857 - val_loss: 5.7966 - val_accuracy: 0.6306\n",
            "Epoch 270/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2015 - accuracy: 0.8857 - val_loss: 5.9225 - val_accuracy: 0.6036\n",
            "Epoch 271/1000\n",
            "28/28 [==============================] - 3s 112ms/step - loss: 0.2025 - accuracy: 0.8846 - val_loss: 6.0322 - val_accuracy: 0.6261\n",
            "Epoch 272/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2026 - accuracy: 0.8903 - val_loss: 6.0855 - val_accuracy: 0.6261\n",
            "Epoch 273/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2048 - accuracy: 0.8880 - val_loss: 6.1002 - val_accuracy: 0.6306\n",
            "Epoch 274/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2010 - accuracy: 0.8846 - val_loss: 6.1642 - val_accuracy: 0.6216\n",
            "Epoch 275/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2051 - accuracy: 0.8846 - val_loss: 6.1351 - val_accuracy: 0.6261\n",
            "Epoch 276/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2035 - accuracy: 0.8891 - val_loss: 5.9373 - val_accuracy: 0.6216\n",
            "Epoch 277/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2007 - accuracy: 0.8937 - val_loss: 6.1503 - val_accuracy: 0.6306\n",
            "Epoch 278/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2027 - accuracy: 0.8846 - val_loss: 6.4969 - val_accuracy: 0.6306\n",
            "Epoch 279/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2045 - accuracy: 0.8880 - val_loss: 6.4574 - val_accuracy: 0.6216\n",
            "Epoch 280/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.1985 - accuracy: 0.8891 - val_loss: 6.6956 - val_accuracy: 0.6216\n",
            "Epoch 281/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2031 - accuracy: 0.8880 - val_loss: 6.7869 - val_accuracy: 0.6306\n",
            "Epoch 282/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2056 - accuracy: 0.8903 - val_loss: 6.6554 - val_accuracy: 0.6306\n",
            "Epoch 283/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2001 - accuracy: 0.8835 - val_loss: 6.8200 - val_accuracy: 0.6261\n",
            "Epoch 284/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.1980 - accuracy: 0.8925 - val_loss: 6.7494 - val_accuracy: 0.6216\n",
            "Epoch 285/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.1997 - accuracy: 0.8925 - val_loss: 6.7618 - val_accuracy: 0.6441\n",
            "Epoch 286/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2052 - accuracy: 0.8857 - val_loss: 6.7116 - val_accuracy: 0.6306\n",
            "Epoch 287/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2053 - accuracy: 0.8925 - val_loss: 6.7437 - val_accuracy: 0.6261\n",
            "Epoch 288/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2098 - accuracy: 0.8869 - val_loss: 6.6703 - val_accuracy: 0.5991\n",
            "Epoch 289/1000\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.2027 - accuracy: 0.8891 - val_loss: 6.4648 - val_accuracy: 0.6216\n",
            "Epoch 290/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2038 - accuracy: 0.8914 - val_loss: 6.3860 - val_accuracy: 0.6171\n",
            "Epoch 291/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2008 - accuracy: 0.8903 - val_loss: 6.5887 - val_accuracy: 0.6171\n",
            "Epoch 292/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2014 - accuracy: 0.8857 - val_loss: 6.6397 - val_accuracy: 0.6036\n",
            "Epoch 293/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.1985 - accuracy: 0.8937 - val_loss: 6.6776 - val_accuracy: 0.6306\n",
            "Epoch 294/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2042 - accuracy: 0.8937 - val_loss: 6.6125 - val_accuracy: 0.6306\n",
            "Epoch 295/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2041 - accuracy: 0.8971 - val_loss: 5.8546 - val_accuracy: 0.5946\n",
            "Epoch 296/1000\n",
            "28/28 [==============================] - 3s 121ms/step - loss: 0.2010 - accuracy: 0.9005 - val_loss: 6.0862 - val_accuracy: 0.6081\n",
            "Epoch 297/1000\n",
            "28/28 [==============================] - 3s 116ms/step - loss: 0.2091 - accuracy: 0.8835 - val_loss: 6.2634 - val_accuracy: 0.5856\n",
            "Epoch 298/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2022 - accuracy: 0.8812 - val_loss: 6.2062 - val_accuracy: 0.6171\n",
            "Epoch 299/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2112 - accuracy: 0.8801 - val_loss: 6.3071 - val_accuracy: 0.6306\n",
            "Epoch 300/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2039 - accuracy: 0.8925 - val_loss: 6.4468 - val_accuracy: 0.5991\n",
            "Epoch 301/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2008 - accuracy: 0.8891 - val_loss: 6.5054 - val_accuracy: 0.5991\n",
            "Epoch 302/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2036 - accuracy: 0.8857 - val_loss: 6.3455 - val_accuracy: 0.6261\n",
            "Epoch 303/1000\n",
            "28/28 [==============================] - 3s 111ms/step - loss: 0.2026 - accuracy: 0.8869 - val_loss: 6.5487 - val_accuracy: 0.6261\n",
            "Epoch 304/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2039 - accuracy: 0.8937 - val_loss: 6.7262 - val_accuracy: 0.6126\n",
            "Epoch 305/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2040 - accuracy: 0.8846 - val_loss: 6.8386 - val_accuracy: 0.6216\n",
            "Epoch 306/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2062 - accuracy: 0.8835 - val_loss: 7.1330 - val_accuracy: 0.6036\n",
            "Epoch 307/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2032 - accuracy: 0.8903 - val_loss: 7.1129 - val_accuracy: 0.6171\n",
            "Epoch 308/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2067 - accuracy: 0.8914 - val_loss: 6.7222 - val_accuracy: 0.6306\n",
            "Epoch 309/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2048 - accuracy: 0.8824 - val_loss: 6.6339 - val_accuracy: 0.5901\n",
            "Epoch 310/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.1986 - accuracy: 0.8857 - val_loss: 6.5996 - val_accuracy: 0.6036\n",
            "Epoch 311/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2007 - accuracy: 0.8846 - val_loss: 6.6192 - val_accuracy: 0.6081\n",
            "Epoch 312/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2026 - accuracy: 0.8914 - val_loss: 6.9807 - val_accuracy: 0.6081\n",
            "Epoch 313/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2032 - accuracy: 0.8835 - val_loss: 6.6273 - val_accuracy: 0.6081\n",
            "Epoch 314/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2005 - accuracy: 0.8869 - val_loss: 6.7633 - val_accuracy: 0.6081\n",
            "Epoch 315/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1993 - accuracy: 0.8937 - val_loss: 6.7826 - val_accuracy: 0.6351\n",
            "Epoch 316/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.2032 - accuracy: 0.8846 - val_loss: 6.5929 - val_accuracy: 0.6036\n",
            "Epoch 317/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2017 - accuracy: 0.8948 - val_loss: 6.4361 - val_accuracy: 0.6171\n",
            "Epoch 318/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2033 - accuracy: 0.8722 - val_loss: 6.5316 - val_accuracy: 0.6081\n",
            "Epoch 319/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.1997 - accuracy: 0.8914 - val_loss: 6.6203 - val_accuracy: 0.6081\n",
            "Epoch 320/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2068 - accuracy: 0.8801 - val_loss: 6.5987 - val_accuracy: 0.6171\n",
            "Epoch 321/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2058 - accuracy: 0.8891 - val_loss: 6.6158 - val_accuracy: 0.6126\n",
            "Epoch 322/1000\n",
            "28/28 [==============================] - 3s 108ms/step - loss: 0.2042 - accuracy: 0.8891 - val_loss: 6.8418 - val_accuracy: 0.6171\n",
            "Epoch 323/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2032 - accuracy: 0.8846 - val_loss: 7.0584 - val_accuracy: 0.6306\n",
            "Epoch 324/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2056 - accuracy: 0.8869 - val_loss: 6.9780 - val_accuracy: 0.6126\n",
            "Epoch 325/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2052 - accuracy: 0.8880 - val_loss: 6.7397 - val_accuracy: 0.5991\n",
            "Epoch 326/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2017 - accuracy: 0.8869 - val_loss: 6.8182 - val_accuracy: 0.6126\n",
            "Epoch 327/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2044 - accuracy: 0.8869 - val_loss: 6.7625 - val_accuracy: 0.6081\n",
            "Epoch 328/1000\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.2014 - accuracy: 0.8891 - val_loss: 6.9640 - val_accuracy: 0.6216\n",
            "Epoch 329/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.2007 - accuracy: 0.8903 - val_loss: 7.2262 - val_accuracy: 0.6036\n",
            "Epoch 330/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2069 - accuracy: 0.8891 - val_loss: 6.9335 - val_accuracy: 0.6261\n",
            "Epoch 331/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2069 - accuracy: 0.8891 - val_loss: 6.9346 - val_accuracy: 0.6261\n",
            "Epoch 332/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2008 - accuracy: 0.8903 - val_loss: 7.0216 - val_accuracy: 0.6306\n",
            "Epoch 333/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2013 - accuracy: 0.8857 - val_loss: 7.0804 - val_accuracy: 0.6081\n",
            "Epoch 334/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2044 - accuracy: 0.8903 - val_loss: 6.9114 - val_accuracy: 0.6036\n",
            "Epoch 335/1000\n",
            "28/28 [==============================] - 3s 110ms/step - loss: 0.2062 - accuracy: 0.8824 - val_loss: 6.4059 - val_accuracy: 0.6171\n",
            "Epoch 336/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2005 - accuracy: 0.8914 - val_loss: 6.5761 - val_accuracy: 0.6126\n",
            "Epoch 337/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2059 - accuracy: 0.8801 - val_loss: 6.1995 - val_accuracy: 0.6171\n",
            "Epoch 338/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2011 - accuracy: 0.8903 - val_loss: 6.3656 - val_accuracy: 0.6306\n",
            "Epoch 339/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2069 - accuracy: 0.8790 - val_loss: 6.4875 - val_accuracy: 0.6171\n",
            "Epoch 340/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2029 - accuracy: 0.8869 - val_loss: 6.5595 - val_accuracy: 0.6081\n",
            "Epoch 341/1000\n",
            "28/28 [==============================] - 3s 108ms/step - loss: 0.2017 - accuracy: 0.8948 - val_loss: 6.5445 - val_accuracy: 0.6216\n",
            "Epoch 342/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2090 - accuracy: 0.8846 - val_loss: 6.6292 - val_accuracy: 0.6351\n",
            "Epoch 343/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2042 - accuracy: 0.8869 - val_loss: 6.5097 - val_accuracy: 0.6351\n",
            "Epoch 344/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2044 - accuracy: 0.8891 - val_loss: 6.3977 - val_accuracy: 0.6261\n",
            "Epoch 345/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2031 - accuracy: 0.8891 - val_loss: 6.5709 - val_accuracy: 0.6351\n",
            "Epoch 346/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.1987 - accuracy: 0.8914 - val_loss: 6.7146 - val_accuracy: 0.6216\n",
            "Epoch 347/1000\n",
            "28/28 [==============================] - 2s 87ms/step - loss: 0.2038 - accuracy: 0.8880 - val_loss: 6.7214 - val_accuracy: 0.6486\n",
            "Epoch 348/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2059 - accuracy: 0.8903 - val_loss: 6.7413 - val_accuracy: 0.6261\n",
            "Epoch 349/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2038 - accuracy: 0.8812 - val_loss: 6.8352 - val_accuracy: 0.6351\n",
            "Epoch 350/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2017 - accuracy: 0.8914 - val_loss: 6.8759 - val_accuracy: 0.6081\n",
            "Epoch 351/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2049 - accuracy: 0.8812 - val_loss: 6.6940 - val_accuracy: 0.6036\n",
            "Epoch 352/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2061 - accuracy: 0.8903 - val_loss: 6.1240 - val_accuracy: 0.6171\n",
            "Epoch 353/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2011 - accuracy: 0.8778 - val_loss: 6.2443 - val_accuracy: 0.6261\n",
            "Epoch 354/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2024 - accuracy: 0.8959 - val_loss: 7.5686 - val_accuracy: 0.5991\n",
            "Epoch 355/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2113 - accuracy: 0.8846 - val_loss: 6.2220 - val_accuracy: 0.6351\n",
            "Epoch 356/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2019 - accuracy: 0.8880 - val_loss: 6.1210 - val_accuracy: 0.6171\n",
            "Epoch 357/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2003 - accuracy: 0.8937 - val_loss: 6.3803 - val_accuracy: 0.6441\n",
            "Epoch 358/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2058 - accuracy: 0.8903 - val_loss: 6.2406 - val_accuracy: 0.6396\n",
            "Epoch 359/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2022 - accuracy: 0.8891 - val_loss: 6.3263 - val_accuracy: 0.6171\n",
            "Epoch 360/1000\n",
            "28/28 [==============================] - 3s 108ms/step - loss: 0.2038 - accuracy: 0.8880 - val_loss: 6.1191 - val_accuracy: 0.6216\n",
            "Epoch 361/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2068 - accuracy: 0.8846 - val_loss: 6.0032 - val_accuracy: 0.6216\n",
            "Epoch 362/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2054 - accuracy: 0.8846 - val_loss: 6.2199 - val_accuracy: 0.6036\n",
            "Epoch 363/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2156 - accuracy: 0.8857 - val_loss: 6.6236 - val_accuracy: 0.5721\n",
            "Epoch 364/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2561 - accuracy: 0.8903 - val_loss: 5.0942 - val_accuracy: 0.6261\n",
            "Epoch 365/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2081 - accuracy: 0.8857 - val_loss: 5.3458 - val_accuracy: 0.6171\n",
            "Epoch 366/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2055 - accuracy: 0.8846 - val_loss: 5.6079 - val_accuracy: 0.6081\n",
            "Epoch 367/1000\n",
            "28/28 [==============================] - 2s 87ms/step - loss: 0.2012 - accuracy: 0.8846 - val_loss: 5.8420 - val_accuracy: 0.6261\n",
            "Epoch 368/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1996 - accuracy: 0.8903 - val_loss: 5.9429 - val_accuracy: 0.5991\n",
            "Epoch 369/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2138 - accuracy: 0.8891 - val_loss: 5.9196 - val_accuracy: 0.6171\n",
            "Epoch 370/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2087 - accuracy: 0.8812 - val_loss: 5.9593 - val_accuracy: 0.6216\n",
            "Epoch 371/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2107 - accuracy: 0.8756 - val_loss: 5.7866 - val_accuracy: 0.5946\n",
            "Epoch 372/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2041 - accuracy: 0.8937 - val_loss: 5.9773 - val_accuracy: 0.6081\n",
            "Epoch 373/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2062 - accuracy: 0.8846 - val_loss: 6.0105 - val_accuracy: 0.6306\n",
            "Epoch 374/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2080 - accuracy: 0.8891 - val_loss: 5.8789 - val_accuracy: 0.6351\n",
            "Epoch 375/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2065 - accuracy: 0.8835 - val_loss: 5.8689 - val_accuracy: 0.6216\n",
            "Epoch 376/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2172 - accuracy: 0.8790 - val_loss: 6.3321 - val_accuracy: 0.6171\n",
            "Epoch 377/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2078 - accuracy: 0.8857 - val_loss: 5.3277 - val_accuracy: 0.6261\n",
            "Epoch 378/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2055 - accuracy: 0.8846 - val_loss: 5.7261 - val_accuracy: 0.6171\n",
            "Epoch 379/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2058 - accuracy: 0.8835 - val_loss: 5.7095 - val_accuracy: 0.5991\n",
            "Epoch 380/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2083 - accuracy: 0.8824 - val_loss: 5.7915 - val_accuracy: 0.6261\n",
            "Epoch 381/1000\n",
            "28/28 [==============================] - 2s 87ms/step - loss: 0.2030 - accuracy: 0.8914 - val_loss: 5.6829 - val_accuracy: 0.6216\n",
            "Epoch 382/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2194 - accuracy: 0.8891 - val_loss: 5.5406 - val_accuracy: 0.5811\n",
            "Epoch 383/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2083 - accuracy: 0.8846 - val_loss: 5.2724 - val_accuracy: 0.5991\n",
            "Epoch 384/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2074 - accuracy: 0.8801 - val_loss: 5.3254 - val_accuracy: 0.6126\n",
            "Epoch 385/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2152 - accuracy: 0.8903 - val_loss: 5.1557 - val_accuracy: 0.6171\n",
            "Epoch 386/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2058 - accuracy: 0.8869 - val_loss: 5.2639 - val_accuracy: 0.6171\n",
            "Epoch 387/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2045 - accuracy: 0.8869 - val_loss: 5.3716 - val_accuracy: 0.6216\n",
            "Epoch 388/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2041 - accuracy: 0.8891 - val_loss: 5.6202 - val_accuracy: 0.6261\n",
            "Epoch 389/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2020 - accuracy: 0.8812 - val_loss: 5.6771 - val_accuracy: 0.6126\n",
            "Epoch 390/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2001 - accuracy: 0.8869 - val_loss: 5.8163 - val_accuracy: 0.6126\n",
            "Epoch 391/1000\n",
            "28/28 [==============================] - 3s 113ms/step - loss: 0.2038 - accuracy: 0.8903 - val_loss: 5.9371 - val_accuracy: 0.6036\n",
            "Epoch 392/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2067 - accuracy: 0.8812 - val_loss: 5.7142 - val_accuracy: 0.6351\n",
            "Epoch 393/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2148 - accuracy: 0.8857 - val_loss: 6.0695 - val_accuracy: 0.6261\n",
            "Epoch 394/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2130 - accuracy: 0.8824 - val_loss: 5.7446 - val_accuracy: 0.6306\n",
            "Epoch 395/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2048 - accuracy: 0.8835 - val_loss: 5.7908 - val_accuracy: 0.6126\n",
            "Epoch 396/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2010 - accuracy: 0.8835 - val_loss: 5.8656 - val_accuracy: 0.6081\n",
            "Epoch 397/1000\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.2035 - accuracy: 0.8959 - val_loss: 5.9922 - val_accuracy: 0.6036\n",
            "Epoch 398/1000\n",
            "28/28 [==============================] - 2s 86ms/step - loss: 0.2028 - accuracy: 0.8857 - val_loss: 5.9627 - val_accuracy: 0.6081\n",
            "Epoch 399/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2021 - accuracy: 0.8869 - val_loss: 6.0745 - val_accuracy: 0.6036\n",
            "Epoch 400/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2114 - accuracy: 0.8835 - val_loss: 5.2488 - val_accuracy: 0.5946\n",
            "Epoch 401/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2071 - accuracy: 0.8869 - val_loss: 4.9135 - val_accuracy: 0.6216\n",
            "Epoch 402/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2029 - accuracy: 0.8914 - val_loss: 5.1390 - val_accuracy: 0.6126\n",
            "Epoch 403/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2044 - accuracy: 0.8903 - val_loss: 5.2465 - val_accuracy: 0.6261\n",
            "Epoch 404/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2048 - accuracy: 0.8880 - val_loss: 5.2784 - val_accuracy: 0.6081\n",
            "Epoch 405/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2025 - accuracy: 0.8891 - val_loss: 5.2578 - val_accuracy: 0.6171\n",
            "Epoch 406/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2045 - accuracy: 0.8835 - val_loss: 5.3447 - val_accuracy: 0.6171\n",
            "Epoch 407/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2001 - accuracy: 0.8903 - val_loss: 5.4728 - val_accuracy: 0.6171\n",
            "Epoch 408/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2046 - accuracy: 0.8846 - val_loss: 5.3577 - val_accuracy: 0.6171\n",
            "Epoch 409/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2008 - accuracy: 0.8869 - val_loss: 5.4122 - val_accuracy: 0.6441\n",
            "Epoch 410/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2026 - accuracy: 0.8846 - val_loss: 5.4474 - val_accuracy: 0.6171\n",
            "Epoch 411/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1977 - accuracy: 0.8835 - val_loss: 5.4076 - val_accuracy: 0.6171\n",
            "Epoch 412/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2080 - accuracy: 0.8880 - val_loss: 5.4854 - val_accuracy: 0.6171\n",
            "Epoch 413/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.1995 - accuracy: 0.8869 - val_loss: 5.6064 - val_accuracy: 0.6081\n",
            "Epoch 414/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1999 - accuracy: 0.8959 - val_loss: 5.6694 - val_accuracy: 0.6171\n",
            "Epoch 415/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.1989 - accuracy: 0.8925 - val_loss: 5.7355 - val_accuracy: 0.6126\n",
            "Epoch 416/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.2006 - accuracy: 0.8869 - val_loss: 5.7357 - val_accuracy: 0.6171\n",
            "Epoch 417/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.1961 - accuracy: 0.8948 - val_loss: 5.9044 - val_accuracy: 0.6171\n",
            "Epoch 418/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2020 - accuracy: 0.8891 - val_loss: 5.6987 - val_accuracy: 0.6081\n",
            "Epoch 419/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2059 - accuracy: 0.8824 - val_loss: 5.5870 - val_accuracy: 0.6081\n",
            "Epoch 420/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2044 - accuracy: 0.8869 - val_loss: 5.5170 - val_accuracy: 0.6171\n",
            "Epoch 421/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1977 - accuracy: 0.8903 - val_loss: 5.8426 - val_accuracy: 0.6126\n",
            "Epoch 422/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2025 - accuracy: 0.8846 - val_loss: 5.9534 - val_accuracy: 0.6126\n",
            "Epoch 423/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2033 - accuracy: 0.8846 - val_loss: 5.6708 - val_accuracy: 0.6171\n",
            "Epoch 424/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.1991 - accuracy: 0.8891 - val_loss: 5.6865 - val_accuracy: 0.6171\n",
            "Epoch 425/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2025 - accuracy: 0.8824 - val_loss: 5.6145 - val_accuracy: 0.6081\n",
            "Epoch 426/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2046 - accuracy: 0.8824 - val_loss: 5.6738 - val_accuracy: 0.6036\n",
            "Epoch 427/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2034 - accuracy: 0.8903 - val_loss: 5.6839 - val_accuracy: 0.6126\n",
            "Epoch 428/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2036 - accuracy: 0.8880 - val_loss: 5.5940 - val_accuracy: 0.6171\n",
            "Epoch 429/1000\n",
            "28/28 [==============================] - 3s 111ms/step - loss: 0.2036 - accuracy: 0.8824 - val_loss: 5.6580 - val_accuracy: 0.6126\n",
            "Epoch 430/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2055 - accuracy: 0.8903 - val_loss: 5.6642 - val_accuracy: 0.5901\n",
            "Epoch 431/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2025 - accuracy: 0.8925 - val_loss: 5.6251 - val_accuracy: 0.6126\n",
            "Epoch 432/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.1974 - accuracy: 0.8891 - val_loss: 5.7132 - val_accuracy: 0.6216\n",
            "Epoch 433/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2019 - accuracy: 0.8937 - val_loss: 5.7333 - val_accuracy: 0.6036\n",
            "Epoch 434/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2045 - accuracy: 0.8880 - val_loss: 5.5997 - val_accuracy: 0.6171\n",
            "Epoch 435/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.1982 - accuracy: 0.8903 - val_loss: 5.8161 - val_accuracy: 0.6171\n",
            "Epoch 436/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2007 - accuracy: 0.8869 - val_loss: 5.8496 - val_accuracy: 0.6171\n",
            "Epoch 437/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2018 - accuracy: 0.8903 - val_loss: 5.8160 - val_accuracy: 0.6171\n",
            "Epoch 438/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2021 - accuracy: 0.8880 - val_loss: 5.8286 - val_accuracy: 0.6261\n",
            "Epoch 439/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2046 - accuracy: 0.8846 - val_loss: 5.9807 - val_accuracy: 0.6351\n",
            "Epoch 440/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1992 - accuracy: 0.8903 - val_loss: 6.1219 - val_accuracy: 0.6261\n",
            "Epoch 441/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2006 - accuracy: 0.8846 - val_loss: 6.2089 - val_accuracy: 0.6351\n",
            "Epoch 442/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.2004 - accuracy: 0.8914 - val_loss: 6.0676 - val_accuracy: 0.6306\n",
            "Epoch 443/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2014 - accuracy: 0.8857 - val_loss: 6.0092 - val_accuracy: 0.6351\n",
            "Epoch 444/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1992 - accuracy: 0.8914 - val_loss: 6.0356 - val_accuracy: 0.6396\n",
            "Epoch 445/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2013 - accuracy: 0.8880 - val_loss: 6.0232 - val_accuracy: 0.6532\n",
            "Epoch 446/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2028 - accuracy: 0.8903 - val_loss: 5.8246 - val_accuracy: 0.6351\n",
            "Epoch 447/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.2023 - accuracy: 0.8914 - val_loss: 5.7691 - val_accuracy: 0.6261\n",
            "Epoch 448/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.1990 - accuracy: 0.8937 - val_loss: 5.8814 - val_accuracy: 0.6306\n",
            "Epoch 449/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2034 - accuracy: 0.8835 - val_loss: 5.7852 - val_accuracy: 0.6261\n",
            "Epoch 450/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2039 - accuracy: 0.8846 - val_loss: 5.7984 - val_accuracy: 0.5946\n",
            "Epoch 451/1000\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.2005 - accuracy: 0.8891 - val_loss: 5.6909 - val_accuracy: 0.6171\n",
            "Epoch 452/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1993 - accuracy: 0.8937 - val_loss: 5.6694 - val_accuracy: 0.6306\n",
            "Epoch 453/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2047 - accuracy: 0.8824 - val_loss: 5.6568 - val_accuracy: 0.6171\n",
            "Epoch 454/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2050 - accuracy: 0.8880 - val_loss: 5.6553 - val_accuracy: 0.6081\n",
            "Epoch 455/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2044 - accuracy: 0.8869 - val_loss: 5.6384 - val_accuracy: 0.6171\n",
            "Epoch 456/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2002 - accuracy: 0.8835 - val_loss: 5.8140 - val_accuracy: 0.6216\n",
            "Epoch 457/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2016 - accuracy: 0.8857 - val_loss: 5.6557 - val_accuracy: 0.6306\n",
            "Epoch 458/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2037 - accuracy: 0.8880 - val_loss: 5.7283 - val_accuracy: 0.6351\n",
            "Epoch 459/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2032 - accuracy: 0.8971 - val_loss: 5.7234 - val_accuracy: 0.6216\n",
            "Epoch 460/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2052 - accuracy: 0.8812 - val_loss: 5.7625 - val_accuracy: 0.6216\n",
            "Epoch 461/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.2031 - accuracy: 0.8959 - val_loss: 5.6648 - val_accuracy: 0.6216\n",
            "Epoch 462/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2047 - accuracy: 0.8824 - val_loss: 5.9159 - val_accuracy: 0.6351\n",
            "Epoch 463/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2018 - accuracy: 0.8914 - val_loss: 6.0004 - val_accuracy: 0.6351\n",
            "Epoch 464/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2052 - accuracy: 0.8857 - val_loss: 6.0524 - val_accuracy: 0.6532\n",
            "Epoch 465/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2040 - accuracy: 0.8869 - val_loss: 6.1152 - val_accuracy: 0.6351\n",
            "Epoch 466/1000\n",
            "28/28 [==============================] - 3s 120ms/step - loss: 0.2023 - accuracy: 0.8914 - val_loss: 6.0848 - val_accuracy: 0.6351\n",
            "Epoch 467/1000\n",
            "28/28 [==============================] - 3s 116ms/step - loss: 0.2040 - accuracy: 0.8880 - val_loss: 6.1569 - val_accuracy: 0.6441\n",
            "Epoch 468/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2047 - accuracy: 0.8914 - val_loss: 6.0567 - val_accuracy: 0.6441\n",
            "Epoch 469/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2014 - accuracy: 0.8937 - val_loss: 6.0924 - val_accuracy: 0.6261\n",
            "Epoch 470/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1999 - accuracy: 0.8914 - val_loss: 6.0499 - val_accuracy: 0.6216\n",
            "Epoch 471/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2024 - accuracy: 0.8824 - val_loss: 6.0626 - val_accuracy: 0.6261\n",
            "Epoch 472/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2025 - accuracy: 0.8982 - val_loss: 6.1185 - val_accuracy: 0.6126\n",
            "Epoch 473/1000\n",
            "28/28 [==============================] - 3s 111ms/step - loss: 0.2012 - accuracy: 0.8914 - val_loss: 5.9973 - val_accuracy: 0.6171\n",
            "Epoch 474/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2017 - accuracy: 0.8891 - val_loss: 6.1232 - val_accuracy: 0.6396\n",
            "Epoch 475/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2041 - accuracy: 0.8891 - val_loss: 6.1954 - val_accuracy: 0.6216\n",
            "Epoch 476/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2035 - accuracy: 0.8846 - val_loss: 6.1967 - val_accuracy: 0.6216\n",
            "Epoch 477/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2002 - accuracy: 0.8925 - val_loss: 6.2006 - val_accuracy: 0.6261\n",
            "Epoch 478/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.1996 - accuracy: 0.8846 - val_loss: 6.3646 - val_accuracy: 0.6216\n",
            "Epoch 479/1000\n",
            "28/28 [==============================] - 3s 119ms/step - loss: 0.2039 - accuracy: 0.8869 - val_loss: 6.3252 - val_accuracy: 0.6171\n",
            "Epoch 480/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2032 - accuracy: 0.8891 - val_loss: 6.3156 - val_accuracy: 0.6171\n",
            "Epoch 481/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2023 - accuracy: 0.8880 - val_loss: 6.2637 - val_accuracy: 0.6171\n",
            "Epoch 482/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.1983 - accuracy: 0.8880 - val_loss: 6.1005 - val_accuracy: 0.6171\n",
            "Epoch 483/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2010 - accuracy: 0.8812 - val_loss: 6.1649 - val_accuracy: 0.6306\n",
            "Epoch 484/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2011 - accuracy: 0.8846 - val_loss: 6.3085 - val_accuracy: 0.6126\n",
            "Epoch 485/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2035 - accuracy: 0.8857 - val_loss: 6.2643 - val_accuracy: 0.6126\n",
            "Epoch 486/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.2044 - accuracy: 0.8869 - val_loss: 6.0001 - val_accuracy: 0.6081\n",
            "Epoch 487/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2040 - accuracy: 0.8937 - val_loss: 6.2108 - val_accuracy: 0.6216\n",
            "Epoch 488/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2035 - accuracy: 0.8857 - val_loss: 6.0504 - val_accuracy: 0.6261\n",
            "Epoch 489/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.1989 - accuracy: 0.8948 - val_loss: 6.1211 - val_accuracy: 0.6351\n",
            "Epoch 490/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2037 - accuracy: 0.8914 - val_loss: 6.2373 - val_accuracy: 0.6306\n",
            "Epoch 491/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.1986 - accuracy: 0.8925 - val_loss: 6.2547 - val_accuracy: 0.6396\n",
            "Epoch 492/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.2025 - accuracy: 0.8914 - val_loss: 6.3667 - val_accuracy: 0.6441\n",
            "Epoch 493/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2017 - accuracy: 0.8869 - val_loss: 6.4240 - val_accuracy: 0.6216\n",
            "Epoch 494/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2031 - accuracy: 0.8857 - val_loss: 6.7450 - val_accuracy: 0.6261\n",
            "Epoch 495/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.1990 - accuracy: 0.8891 - val_loss: 6.9001 - val_accuracy: 0.6171\n",
            "Epoch 496/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2045 - accuracy: 0.8880 - val_loss: 6.8484 - val_accuracy: 0.6126\n",
            "Epoch 497/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.2020 - accuracy: 0.8824 - val_loss: 6.6374 - val_accuracy: 0.6216\n",
            "Epoch 498/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2053 - accuracy: 0.8857 - val_loss: 6.6168 - val_accuracy: 0.6216\n",
            "Epoch 499/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2185 - accuracy: 0.8937 - val_loss: 6.2113 - val_accuracy: 0.6351\n",
            "Epoch 500/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2037 - accuracy: 0.8880 - val_loss: 6.1568 - val_accuracy: 0.6396\n",
            "Epoch 501/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2028 - accuracy: 0.8903 - val_loss: 6.3107 - val_accuracy: 0.6216\n",
            "Epoch 502/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2019 - accuracy: 0.8891 - val_loss: 6.3028 - val_accuracy: 0.6081\n",
            "Epoch 503/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2017 - accuracy: 0.8801 - val_loss: 6.2356 - val_accuracy: 0.6036\n",
            "Epoch 504/1000\n",
            "28/28 [==============================] - 3s 112ms/step - loss: 0.2038 - accuracy: 0.8869 - val_loss: 6.4713 - val_accuracy: 0.6351\n",
            "Epoch 505/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2044 - accuracy: 0.8937 - val_loss: 6.5707 - val_accuracy: 0.6351\n",
            "Epoch 506/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2032 - accuracy: 0.8903 - val_loss: 6.5416 - val_accuracy: 0.6261\n",
            "Epoch 507/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2013 - accuracy: 0.8880 - val_loss: 6.4740 - val_accuracy: 0.6261\n",
            "Epoch 508/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2036 - accuracy: 0.8880 - val_loss: 6.4551 - val_accuracy: 0.6081\n",
            "Epoch 509/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2036 - accuracy: 0.8869 - val_loss: 6.5694 - val_accuracy: 0.6306\n",
            "Epoch 510/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2055 - accuracy: 0.8857 - val_loss: 6.5651 - val_accuracy: 0.6081\n",
            "Epoch 511/1000\n",
            "28/28 [==============================] - 2s 87ms/step - loss: 0.2080 - accuracy: 0.8948 - val_loss: 6.6909 - val_accuracy: 0.6081\n",
            "Epoch 512/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2046 - accuracy: 0.8914 - val_loss: 6.7863 - val_accuracy: 0.5991\n",
            "Epoch 513/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2010 - accuracy: 0.8880 - val_loss: 7.1210 - val_accuracy: 0.6306\n",
            "Epoch 514/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2071 - accuracy: 0.8914 - val_loss: 6.8882 - val_accuracy: 0.6216\n",
            "Epoch 515/1000\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.2019 - accuracy: 0.8846 - val_loss: 6.9650 - val_accuracy: 0.6306\n",
            "Epoch 516/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2005 - accuracy: 0.8835 - val_loss: 6.9308 - val_accuracy: 0.6306\n",
            "Epoch 517/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.1995 - accuracy: 0.8903 - val_loss: 6.8675 - val_accuracy: 0.6216\n",
            "Epoch 518/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2025 - accuracy: 0.8937 - val_loss: 6.9864 - val_accuracy: 0.6216\n",
            "Epoch 519/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2019 - accuracy: 0.8914 - val_loss: 6.9591 - val_accuracy: 0.6171\n",
            "Epoch 520/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2018 - accuracy: 0.8903 - val_loss: 7.0589 - val_accuracy: 0.6261\n",
            "Epoch 521/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2161 - accuracy: 0.8857 - val_loss: 6.5156 - val_accuracy: 0.6171\n",
            "Epoch 522/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2017 - accuracy: 0.8869 - val_loss: 6.2304 - val_accuracy: 0.6171\n",
            "Epoch 523/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2045 - accuracy: 0.8891 - val_loss: 6.6348 - val_accuracy: 0.6126\n",
            "Epoch 524/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2022 - accuracy: 0.8937 - val_loss: 6.6904 - val_accuracy: 0.6216\n",
            "Epoch 525/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2047 - accuracy: 0.8857 - val_loss: 6.6520 - val_accuracy: 0.6261\n",
            "Epoch 526/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2033 - accuracy: 0.8857 - val_loss: 6.5101 - val_accuracy: 0.6126\n",
            "Epoch 527/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2026 - accuracy: 0.8846 - val_loss: 6.2258 - val_accuracy: 0.6306\n",
            "Epoch 528/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2076 - accuracy: 0.8891 - val_loss: 6.1209 - val_accuracy: 0.6171\n",
            "Epoch 529/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2161 - accuracy: 0.8790 - val_loss: 6.0056 - val_accuracy: 0.6306\n",
            "Epoch 530/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2003 - accuracy: 0.8914 - val_loss: 6.1218 - val_accuracy: 0.6081\n",
            "Epoch 531/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2069 - accuracy: 0.8846 - val_loss: 6.2929 - val_accuracy: 0.6261\n",
            "Epoch 532/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2012 - accuracy: 0.8925 - val_loss: 6.2510 - val_accuracy: 0.6171\n",
            "Epoch 533/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2012 - accuracy: 0.8948 - val_loss: 6.4019 - val_accuracy: 0.6126\n",
            "Epoch 534/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2027 - accuracy: 0.8891 - val_loss: 6.3766 - val_accuracy: 0.6261\n",
            "Epoch 535/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2029 - accuracy: 0.8891 - val_loss: 6.0998 - val_accuracy: 0.6261\n",
            "Epoch 536/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.2146 - accuracy: 0.8778 - val_loss: 6.1998 - val_accuracy: 0.5991\n",
            "Epoch 537/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2006 - accuracy: 0.8937 - val_loss: 6.2414 - val_accuracy: 0.6036\n",
            "Epoch 538/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2073 - accuracy: 0.8835 - val_loss: 6.2475 - val_accuracy: 0.6216\n",
            "Epoch 539/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2050 - accuracy: 0.8846 - val_loss: 6.3824 - val_accuracy: 0.6261\n",
            "Epoch 540/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2132 - accuracy: 0.8824 - val_loss: 5.9580 - val_accuracy: 0.6171\n",
            "Epoch 541/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2055 - accuracy: 0.8835 - val_loss: 5.6002 - val_accuracy: 0.6081\n",
            "Epoch 542/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2039 - accuracy: 0.8891 - val_loss: 6.0670 - val_accuracy: 0.6171\n",
            "Epoch 543/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2020 - accuracy: 0.8880 - val_loss: 6.0873 - val_accuracy: 0.6126\n",
            "Epoch 544/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2045 - accuracy: 0.8824 - val_loss: 6.1767 - val_accuracy: 0.6081\n",
            "Epoch 545/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2023 - accuracy: 0.8880 - val_loss: 6.1770 - val_accuracy: 0.6081\n",
            "Epoch 546/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2032 - accuracy: 0.8880 - val_loss: 6.1457 - val_accuracy: 0.5946\n",
            "Epoch 547/1000\n",
            "28/28 [==============================] - 3s 97ms/step - loss: 0.2066 - accuracy: 0.8959 - val_loss: 6.0903 - val_accuracy: 0.6036\n",
            "Epoch 548/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.2032 - accuracy: 0.8891 - val_loss: 6.2316 - val_accuracy: 0.6036\n",
            "Epoch 549/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2064 - accuracy: 0.8846 - val_loss: 6.3518 - val_accuracy: 0.6081\n",
            "Epoch 550/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2053 - accuracy: 0.8846 - val_loss: 6.2798 - val_accuracy: 0.6171\n",
            "Epoch 551/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2008 - accuracy: 0.8925 - val_loss: 6.4054 - val_accuracy: 0.6171\n",
            "Epoch 552/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2010 - accuracy: 0.8937 - val_loss: 6.5154 - val_accuracy: 0.6261\n",
            "Epoch 553/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2069 - accuracy: 0.8914 - val_loss: 6.5385 - val_accuracy: 0.6171\n",
            "Epoch 554/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2032 - accuracy: 0.8869 - val_loss: 6.6211 - val_accuracy: 0.6261\n",
            "Epoch 555/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2016 - accuracy: 0.8914 - val_loss: 6.7575 - val_accuracy: 0.6171\n",
            "Epoch 556/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2065 - accuracy: 0.8880 - val_loss: 6.5806 - val_accuracy: 0.6396\n",
            "Epoch 557/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.1997 - accuracy: 0.8880 - val_loss: 6.7911 - val_accuracy: 0.6261\n",
            "Epoch 558/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2015 - accuracy: 0.8925 - val_loss: 7.0533 - val_accuracy: 0.6081\n",
            "Epoch 559/1000\n",
            "28/28 [==============================] - 3s 111ms/step - loss: 0.2047 - accuracy: 0.8880 - val_loss: 6.8810 - val_accuracy: 0.6036\n",
            "Epoch 560/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2062 - accuracy: 0.8948 - val_loss: 6.5788 - val_accuracy: 0.6351\n",
            "Epoch 561/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2024 - accuracy: 0.8959 - val_loss: 6.7154 - val_accuracy: 0.6036\n",
            "Epoch 562/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2052 - accuracy: 0.8812 - val_loss: 6.7786 - val_accuracy: 0.5991\n",
            "Epoch 563/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.1972 - accuracy: 0.8937 - val_loss: 6.9286 - val_accuracy: 0.6216\n",
            "Epoch 564/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2022 - accuracy: 0.8903 - val_loss: 6.9333 - val_accuracy: 0.6171\n",
            "Epoch 565/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.2085 - accuracy: 0.8925 - val_loss: 6.5550 - val_accuracy: 0.6171\n",
            "Epoch 566/1000\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.2086 - accuracy: 0.8835 - val_loss: 6.2570 - val_accuracy: 0.6306\n",
            "Epoch 567/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2039 - accuracy: 0.8914 - val_loss: 6.5792 - val_accuracy: 0.6351\n",
            "Epoch 568/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2122 - accuracy: 0.8869 - val_loss: 6.5790 - val_accuracy: 0.6396\n",
            "Epoch 569/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2052 - accuracy: 0.8903 - val_loss: 6.5237 - val_accuracy: 0.6126\n",
            "Epoch 570/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2036 - accuracy: 0.8857 - val_loss: 6.5697 - val_accuracy: 0.5901\n",
            "Epoch 571/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2016 - accuracy: 0.8869 - val_loss: 6.5889 - val_accuracy: 0.5901\n",
            "Epoch 572/1000\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.2136 - accuracy: 0.8880 - val_loss: 6.2693 - val_accuracy: 0.5991\n",
            "Epoch 573/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2032 - accuracy: 0.8778 - val_loss: 5.8259 - val_accuracy: 0.5856\n",
            "Epoch 574/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2040 - accuracy: 0.8891 - val_loss: 6.1890 - val_accuracy: 0.5901\n",
            "Epoch 575/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2023 - accuracy: 0.8914 - val_loss: 6.4065 - val_accuracy: 0.6216\n",
            "Epoch 576/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2072 - accuracy: 0.8812 - val_loss: 6.5888 - val_accuracy: 0.6306\n",
            "Epoch 577/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2026 - accuracy: 0.8903 - val_loss: 6.4735 - val_accuracy: 0.6126\n",
            "Epoch 578/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2031 - accuracy: 0.8835 - val_loss: 6.5829 - val_accuracy: 0.5811\n",
            "Epoch 579/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2048 - accuracy: 0.8857 - val_loss: 6.5245 - val_accuracy: 0.6036\n",
            "Epoch 580/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2055 - accuracy: 0.8857 - val_loss: 6.8482 - val_accuracy: 0.6261\n",
            "Epoch 581/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2242 - accuracy: 0.8801 - val_loss: 6.5434 - val_accuracy: 0.6216\n",
            "Epoch 582/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2046 - accuracy: 0.8790 - val_loss: 6.2192 - val_accuracy: 0.6126\n",
            "Epoch 583/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2036 - accuracy: 0.8824 - val_loss: 6.2939 - val_accuracy: 0.6036\n",
            "Epoch 584/1000\n",
            "28/28 [==============================] - 3s 111ms/step - loss: 0.2046 - accuracy: 0.8891 - val_loss: 6.3543 - val_accuracy: 0.6216\n",
            "Epoch 585/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2003 - accuracy: 0.8937 - val_loss: 6.4208 - val_accuracy: 0.6126\n",
            "Epoch 586/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2131 - accuracy: 0.8824 - val_loss: 6.3214 - val_accuracy: 0.5946\n",
            "Epoch 587/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2117 - accuracy: 0.8959 - val_loss: 6.2995 - val_accuracy: 0.5991\n",
            "Epoch 588/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2044 - accuracy: 0.8937 - val_loss: 6.4785 - val_accuracy: 0.5901\n",
            "Epoch 589/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2029 - accuracy: 0.8903 - val_loss: 6.8085 - val_accuracy: 0.5811\n",
            "Epoch 590/1000\n",
            "28/28 [==============================] - 3s 111ms/step - loss: 0.2075 - accuracy: 0.8846 - val_loss: 6.7922 - val_accuracy: 0.5811\n",
            "Epoch 591/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2001 - accuracy: 0.8903 - val_loss: 6.6826 - val_accuracy: 0.5991\n",
            "Epoch 592/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2038 - accuracy: 0.8891 - val_loss: 6.6051 - val_accuracy: 0.5946\n",
            "Epoch 593/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1998 - accuracy: 0.8869 - val_loss: 6.8494 - val_accuracy: 0.6036\n",
            "Epoch 594/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2029 - accuracy: 0.8869 - val_loss: 6.9659 - val_accuracy: 0.6036\n",
            "Epoch 595/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.1999 - accuracy: 0.8937 - val_loss: 7.1334 - val_accuracy: 0.5946\n",
            "Epoch 596/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2085 - accuracy: 0.8971 - val_loss: 7.0069 - val_accuracy: 0.6036\n",
            "Epoch 597/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.1976 - accuracy: 0.8959 - val_loss: 7.0438 - val_accuracy: 0.6171\n",
            "Epoch 598/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2033 - accuracy: 0.8846 - val_loss: 6.9114 - val_accuracy: 0.6126\n",
            "Epoch 599/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2053 - accuracy: 0.8914 - val_loss: 6.7755 - val_accuracy: 0.5901\n",
            "Epoch 600/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2036 - accuracy: 0.8925 - val_loss: 6.5300 - val_accuracy: 0.6126\n",
            "Epoch 601/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2000 - accuracy: 0.8880 - val_loss: 6.5712 - val_accuracy: 0.5856\n",
            "Epoch 602/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.1997 - accuracy: 0.8948 - val_loss: 6.7094 - val_accuracy: 0.5946\n",
            "Epoch 603/1000\n",
            "28/28 [==============================] - 3s 88ms/step - loss: 0.2025 - accuracy: 0.8914 - val_loss: 6.7570 - val_accuracy: 0.6081\n",
            "Epoch 604/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2014 - accuracy: 0.8790 - val_loss: 6.9165 - val_accuracy: 0.5991\n",
            "Epoch 605/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2044 - accuracy: 0.8914 - val_loss: 7.1108 - val_accuracy: 0.6036\n",
            "Epoch 606/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2003 - accuracy: 0.8903 - val_loss: 7.1682 - val_accuracy: 0.6036\n",
            "Epoch 607/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2053 - accuracy: 0.8880 - val_loss: 7.0540 - val_accuracy: 0.5901\n",
            "Epoch 608/1000\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.1969 - accuracy: 0.8857 - val_loss: 7.0345 - val_accuracy: 0.5991\n",
            "Epoch 609/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2034 - accuracy: 0.8869 - val_loss: 7.0352 - val_accuracy: 0.5901\n",
            "Epoch 610/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2063 - accuracy: 0.8869 - val_loss: 6.8382 - val_accuracy: 0.5946\n",
            "Epoch 611/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2019 - accuracy: 0.8869 - val_loss: 6.9812 - val_accuracy: 0.5946\n",
            "Epoch 612/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2042 - accuracy: 0.8857 - val_loss: 7.1296 - val_accuracy: 0.5946\n",
            "Epoch 613/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2003 - accuracy: 0.8937 - val_loss: 7.3294 - val_accuracy: 0.5766\n",
            "Epoch 614/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2143 - accuracy: 0.8925 - val_loss: 6.4480 - val_accuracy: 0.5856\n",
            "Epoch 615/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.1986 - accuracy: 0.8914 - val_loss: 6.6236 - val_accuracy: 0.5901\n",
            "Epoch 616/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2017 - accuracy: 0.8891 - val_loss: 6.6037 - val_accuracy: 0.5991\n",
            "Epoch 617/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2024 - accuracy: 0.8891 - val_loss: 6.6951 - val_accuracy: 0.5946\n",
            "Epoch 618/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2016 - accuracy: 0.8925 - val_loss: 6.7689 - val_accuracy: 0.5991\n",
            "Epoch 619/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2061 - accuracy: 0.8846 - val_loss: 6.9247 - val_accuracy: 0.6081\n",
            "Epoch 620/1000\n",
            "28/28 [==============================] - 2s 79ms/step - loss: 0.2001 - accuracy: 0.8812 - val_loss: 6.9255 - val_accuracy: 0.6036\n",
            "Epoch 621/1000\n",
            "28/28 [==============================] - 3s 115ms/step - loss: 0.1997 - accuracy: 0.8846 - val_loss: 7.0617 - val_accuracy: 0.6126\n",
            "Epoch 622/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2072 - accuracy: 0.8846 - val_loss: 6.9826 - val_accuracy: 0.6036\n",
            "Epoch 623/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2005 - accuracy: 0.8914 - val_loss: 7.0242 - val_accuracy: 0.6036\n",
            "Epoch 624/1000\n",
            "28/28 [==============================] - 2s 69ms/step - loss: 0.2036 - accuracy: 0.8880 - val_loss: 7.0412 - val_accuracy: 0.6261\n",
            "Epoch 625/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2022 - accuracy: 0.8857 - val_loss: 7.1025 - val_accuracy: 0.5811\n",
            "Epoch 626/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2012 - accuracy: 0.8914 - val_loss: 6.9303 - val_accuracy: 0.5856\n",
            "Epoch 627/1000\n",
            "28/28 [==============================] - 3s 112ms/step - loss: 0.1995 - accuracy: 0.8937 - val_loss: 6.9598 - val_accuracy: 0.5721\n",
            "Epoch 628/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2058 - accuracy: 0.8835 - val_loss: 7.0262 - val_accuracy: 0.5946\n",
            "Epoch 629/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2020 - accuracy: 0.8869 - val_loss: 6.8999 - val_accuracy: 0.5901\n",
            "Epoch 630/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2023 - accuracy: 0.8880 - val_loss: 6.8053 - val_accuracy: 0.5811\n",
            "Epoch 631/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2013 - accuracy: 0.8857 - val_loss: 6.9125 - val_accuracy: 0.5811\n",
            "Epoch 632/1000\n",
            "28/28 [==============================] - 3s 113ms/step - loss: 0.1972 - accuracy: 0.8925 - val_loss: 7.0088 - val_accuracy: 0.5811\n",
            "Epoch 633/1000\n",
            "28/28 [==============================] - 3s 108ms/step - loss: 0.2016 - accuracy: 0.8903 - val_loss: 7.2184 - val_accuracy: 0.5856\n",
            "Epoch 634/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2014 - accuracy: 0.8925 - val_loss: 7.1856 - val_accuracy: 0.5901\n",
            "Epoch 635/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2007 - accuracy: 0.8857 - val_loss: 7.3083 - val_accuracy: 0.5901\n",
            "Epoch 636/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2003 - accuracy: 0.8869 - val_loss: 7.4984 - val_accuracy: 0.5946\n",
            "Epoch 637/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.1975 - accuracy: 0.8959 - val_loss: 7.3170 - val_accuracy: 0.5901\n",
            "Epoch 638/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2009 - accuracy: 0.8903 - val_loss: 7.3422 - val_accuracy: 0.5991\n",
            "Epoch 639/1000\n",
            "28/28 [==============================] - 3s 114ms/step - loss: 0.1999 - accuracy: 0.8937 - val_loss: 7.3266 - val_accuracy: 0.6081\n",
            "Epoch 640/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2031 - accuracy: 0.8846 - val_loss: 7.3691 - val_accuracy: 0.5946\n",
            "Epoch 641/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2007 - accuracy: 0.8903 - val_loss: 7.2903 - val_accuracy: 0.6081\n",
            "Epoch 642/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2057 - accuracy: 0.8801 - val_loss: 7.2273 - val_accuracy: 0.5991\n",
            "Epoch 643/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2032 - accuracy: 0.8835 - val_loss: 7.4617 - val_accuracy: 0.5901\n",
            "Epoch 644/1000\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.2017 - accuracy: 0.8846 - val_loss: 7.3020 - val_accuracy: 0.5721\n",
            "Epoch 645/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2039 - accuracy: 0.8914 - val_loss: 7.2393 - val_accuracy: 0.5946\n",
            "Epoch 646/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2006 - accuracy: 0.8880 - val_loss: 7.2706 - val_accuracy: 0.5946\n",
            "Epoch 647/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2031 - accuracy: 0.8914 - val_loss: 7.3324 - val_accuracy: 0.5946\n",
            "Epoch 648/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2022 - accuracy: 0.8880 - val_loss: 7.6179 - val_accuracy: 0.5946\n",
            "Epoch 649/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2043 - accuracy: 0.8857 - val_loss: 7.5232 - val_accuracy: 0.5946\n",
            "Epoch 650/1000\n",
            "28/28 [==============================] - 2s 88ms/step - loss: 0.2029 - accuracy: 0.8835 - val_loss: 7.0533 - val_accuracy: 0.5856\n",
            "Epoch 651/1000\n",
            "28/28 [==============================] - 3s 115ms/step - loss: 0.1980 - accuracy: 0.8925 - val_loss: 7.3392 - val_accuracy: 0.6036\n",
            "Epoch 652/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2000 - accuracy: 0.8869 - val_loss: 7.2873 - val_accuracy: 0.5901\n",
            "Epoch 653/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.1987 - accuracy: 0.8880 - val_loss: 7.3802 - val_accuracy: 0.6036\n",
            "Epoch 654/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2059 - accuracy: 0.8790 - val_loss: 7.2751 - val_accuracy: 0.5901\n",
            "Epoch 655/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2026 - accuracy: 0.8891 - val_loss: 7.0905 - val_accuracy: 0.5856\n",
            "Epoch 656/1000\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.2026 - accuracy: 0.8869 - val_loss: 7.2209 - val_accuracy: 0.5901\n",
            "Epoch 657/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2022 - accuracy: 0.8891 - val_loss: 7.2632 - val_accuracy: 0.5856\n",
            "Epoch 658/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2034 - accuracy: 0.8903 - val_loss: 7.3107 - val_accuracy: 0.5856\n",
            "Epoch 659/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2180 - accuracy: 0.8846 - val_loss: 7.0560 - val_accuracy: 0.5856\n",
            "Epoch 660/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2065 - accuracy: 0.8880 - val_loss: 6.6900 - val_accuracy: 0.6171\n",
            "Epoch 661/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1977 - accuracy: 0.8914 - val_loss: 7.0660 - val_accuracy: 0.6216\n",
            "Epoch 662/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2030 - accuracy: 0.8824 - val_loss: 7.2678 - val_accuracy: 0.5946\n",
            "Epoch 663/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.1987 - accuracy: 0.8925 - val_loss: 7.4739 - val_accuracy: 0.6171\n",
            "Epoch 664/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2101 - accuracy: 0.8824 - val_loss: 7.2585 - val_accuracy: 0.5946\n",
            "Epoch 665/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2013 - accuracy: 0.8846 - val_loss: 7.3106 - val_accuracy: 0.5856\n",
            "Epoch 666/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2028 - accuracy: 0.8835 - val_loss: 7.1073 - val_accuracy: 0.5901\n",
            "Epoch 667/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2013 - accuracy: 0.8857 - val_loss: 7.2062 - val_accuracy: 0.5811\n",
            "Epoch 668/1000\n",
            "28/28 [==============================] - 3s 97ms/step - loss: 0.2033 - accuracy: 0.8959 - val_loss: 7.2013 - val_accuracy: 0.5901\n",
            "Epoch 669/1000\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.2013 - accuracy: 0.8869 - val_loss: 7.2376 - val_accuracy: 0.5856\n",
            "Epoch 670/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2008 - accuracy: 0.8846 - val_loss: 7.4484 - val_accuracy: 0.5901\n",
            "Epoch 671/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2016 - accuracy: 0.8937 - val_loss: 7.4754 - val_accuracy: 0.5856\n",
            "Epoch 672/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1983 - accuracy: 0.8925 - val_loss: 7.5376 - val_accuracy: 0.5721\n",
            "Epoch 673/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2093 - accuracy: 0.8914 - val_loss: 7.1030 - val_accuracy: 0.5991\n",
            "Epoch 674/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.1989 - accuracy: 0.8880 - val_loss: 7.1791 - val_accuracy: 0.5901\n",
            "Epoch 675/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2023 - accuracy: 0.8835 - val_loss: 7.1605 - val_accuracy: 0.5901\n",
            "Epoch 676/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1993 - accuracy: 0.8903 - val_loss: 7.2976 - val_accuracy: 0.6081\n",
            "Epoch 677/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2013 - accuracy: 0.8937 - val_loss: 7.3172 - val_accuracy: 0.5901\n",
            "Epoch 678/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2038 - accuracy: 0.8880 - val_loss: 7.2963 - val_accuracy: 0.6081\n",
            "Epoch 679/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2018 - accuracy: 0.8846 - val_loss: 7.4015 - val_accuracy: 0.5946\n",
            "Epoch 680/1000\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.2025 - accuracy: 0.8857 - val_loss: 7.3568 - val_accuracy: 0.6216\n",
            "Epoch 681/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2099 - accuracy: 0.8914 - val_loss: 7.5431 - val_accuracy: 0.6081\n",
            "Epoch 682/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2014 - accuracy: 0.8846 - val_loss: 7.2002 - val_accuracy: 0.5991\n",
            "Epoch 683/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1970 - accuracy: 0.8891 - val_loss: 7.2857 - val_accuracy: 0.5811\n",
            "Epoch 684/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2069 - accuracy: 0.8869 - val_loss: 6.9660 - val_accuracy: 0.6036\n",
            "Epoch 685/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2012 - accuracy: 0.8891 - val_loss: 6.9764 - val_accuracy: 0.5946\n",
            "Epoch 686/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2034 - accuracy: 0.8903 - val_loss: 7.2170 - val_accuracy: 0.6261\n",
            "Epoch 687/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2034 - accuracy: 0.8812 - val_loss: 7.3563 - val_accuracy: 0.6171\n",
            "Epoch 688/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2009 - accuracy: 0.8869 - val_loss: 7.5866 - val_accuracy: 0.5946\n",
            "Epoch 689/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2034 - accuracy: 0.8880 - val_loss: 7.5694 - val_accuracy: 0.6036\n",
            "Epoch 690/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2060 - accuracy: 0.8756 - val_loss: 7.4357 - val_accuracy: 0.6126\n",
            "Epoch 691/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2056 - accuracy: 0.8846 - val_loss: 7.6118 - val_accuracy: 0.5991\n",
            "Epoch 692/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2027 - accuracy: 0.8857 - val_loss: 7.6947 - val_accuracy: 0.6081\n",
            "Epoch 693/1000\n",
            "28/28 [==============================] - 3s 109ms/step - loss: 0.2054 - accuracy: 0.8869 - val_loss: 7.1604 - val_accuracy: 0.6036\n",
            "Epoch 694/1000\n",
            "28/28 [==============================] - 2s 79ms/step - loss: 0.1980 - accuracy: 0.8869 - val_loss: 7.2760 - val_accuracy: 0.5901\n",
            "Epoch 695/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2064 - accuracy: 0.8982 - val_loss: 7.3613 - val_accuracy: 0.5901\n",
            "Epoch 696/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2021 - accuracy: 0.8880 - val_loss: 7.3599 - val_accuracy: 0.6036\n",
            "Epoch 697/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2026 - accuracy: 0.8891 - val_loss: 7.5002 - val_accuracy: 0.6126\n",
            "Epoch 698/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2030 - accuracy: 0.8835 - val_loss: 7.4312 - val_accuracy: 0.5946\n",
            "Epoch 699/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2019 - accuracy: 0.8959 - val_loss: 7.4724 - val_accuracy: 0.6081\n",
            "Epoch 700/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2029 - accuracy: 0.8914 - val_loss: 7.6607 - val_accuracy: 0.6126\n",
            "Epoch 701/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2010 - accuracy: 0.8891 - val_loss: 7.6696 - val_accuracy: 0.6081\n",
            "Epoch 702/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2061 - accuracy: 0.8778 - val_loss: 7.6725 - val_accuracy: 0.6036\n",
            "Epoch 703/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.1997 - accuracy: 0.8891 - val_loss: 7.5028 - val_accuracy: 0.6171\n",
            "Epoch 704/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2020 - accuracy: 0.8891 - val_loss: 7.3510 - val_accuracy: 0.6171\n",
            "Epoch 705/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2001 - accuracy: 0.8880 - val_loss: 7.4356 - val_accuracy: 0.5991\n",
            "Epoch 706/1000\n",
            "28/28 [==============================] - 3s 89ms/step - loss: 0.2045 - accuracy: 0.8925 - val_loss: 7.4775 - val_accuracy: 0.5946\n",
            "Epoch 707/1000\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 0.2019 - accuracy: 0.8937 - val_loss: 6.9478 - val_accuracy: 0.6036\n",
            "Epoch 708/1000\n",
            "28/28 [==============================] - 2s 70ms/step - loss: 0.2041 - accuracy: 0.8914 - val_loss: 7.1407 - val_accuracy: 0.5946\n",
            "Epoch 709/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2044 - accuracy: 0.8824 - val_loss: 7.1063 - val_accuracy: 0.5991\n",
            "Epoch 710/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2040 - accuracy: 0.8857 - val_loss: 7.1136 - val_accuracy: 0.5811\n",
            "Epoch 711/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.2035 - accuracy: 0.8824 - val_loss: 7.1212 - val_accuracy: 0.5946\n",
            "Epoch 712/1000\n",
            "28/28 [==============================] - 3s 120ms/step - loss: 0.2002 - accuracy: 0.8971 - val_loss: 7.1791 - val_accuracy: 0.5946\n",
            "Epoch 713/1000\n",
            "28/28 [==============================] - 3s 97ms/step - loss: 0.2012 - accuracy: 0.8914 - val_loss: 7.2018 - val_accuracy: 0.5811\n",
            "Epoch 714/1000\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.2039 - accuracy: 0.8880 - val_loss: 7.0876 - val_accuracy: 0.5991\n",
            "Epoch 715/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.1978 - accuracy: 0.8925 - val_loss: 7.0825 - val_accuracy: 0.5991\n",
            "Epoch 716/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2013 - accuracy: 0.8925 - val_loss: 7.1623 - val_accuracy: 0.6036\n",
            "Epoch 717/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2023 - accuracy: 0.8937 - val_loss: 6.9857 - val_accuracy: 0.6126\n",
            "Epoch 718/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2002 - accuracy: 0.8914 - val_loss: 7.2303 - val_accuracy: 0.5946\n",
            "Epoch 719/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2024 - accuracy: 0.8891 - val_loss: 7.2548 - val_accuracy: 0.6126\n",
            "Epoch 720/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1969 - accuracy: 0.8925 - val_loss: 7.4488 - val_accuracy: 0.6036\n",
            "Epoch 721/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2022 - accuracy: 0.8903 - val_loss: 7.6387 - val_accuracy: 0.6036\n",
            "Epoch 722/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2057 - accuracy: 0.8880 - val_loss: 7.6577 - val_accuracy: 0.6036\n",
            "Epoch 723/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.1949 - accuracy: 0.8812 - val_loss: 7.8026 - val_accuracy: 0.5946\n",
            "Epoch 724/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.2026 - accuracy: 0.8835 - val_loss: 7.7959 - val_accuracy: 0.6081\n",
            "Epoch 725/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2011 - accuracy: 0.8914 - val_loss: 7.7970 - val_accuracy: 0.6036\n",
            "Epoch 726/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2004 - accuracy: 0.8880 - val_loss: 7.7902 - val_accuracy: 0.6036\n",
            "Epoch 727/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2025 - accuracy: 0.8857 - val_loss: 7.7344 - val_accuracy: 0.6036\n",
            "Epoch 728/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2024 - accuracy: 0.8857 - val_loss: 7.6889 - val_accuracy: 0.6126\n",
            "Epoch 729/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2044 - accuracy: 0.8925 - val_loss: 7.6953 - val_accuracy: 0.6036\n",
            "Epoch 730/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.1992 - accuracy: 0.8914 - val_loss: 7.4312 - val_accuracy: 0.5991\n",
            "Epoch 731/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2005 - accuracy: 0.8857 - val_loss: 7.4250 - val_accuracy: 0.6036\n",
            "Epoch 732/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2017 - accuracy: 0.8880 - val_loss: 7.6088 - val_accuracy: 0.5991\n",
            "Epoch 733/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1987 - accuracy: 0.8937 - val_loss: 7.9376 - val_accuracy: 0.6081\n",
            "Epoch 734/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2011 - accuracy: 0.8891 - val_loss: 7.9868 - val_accuracy: 0.5946\n",
            "Epoch 735/1000\n",
            "28/28 [==============================] - 3s 108ms/step - loss: 0.1949 - accuracy: 0.8925 - val_loss: 8.1069 - val_accuracy: 0.6216\n",
            "Epoch 736/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2007 - accuracy: 0.8835 - val_loss: 8.0644 - val_accuracy: 0.6036\n",
            "Epoch 737/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2002 - accuracy: 0.8857 - val_loss: 8.0757 - val_accuracy: 0.6171\n",
            "Epoch 738/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2058 - accuracy: 0.8857 - val_loss: 8.1540 - val_accuracy: 0.6171\n",
            "Epoch 739/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1995 - accuracy: 0.8959 - val_loss: 8.0153 - val_accuracy: 0.6216\n",
            "Epoch 740/1000\n",
            "28/28 [==============================] - 2s 86ms/step - loss: 0.1966 - accuracy: 0.8914 - val_loss: 7.9884 - val_accuracy: 0.6171\n",
            "Epoch 741/1000\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.2083 - accuracy: 0.8778 - val_loss: 7.7825 - val_accuracy: 0.6036\n",
            "Epoch 742/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2025 - accuracy: 0.8857 - val_loss: 7.5272 - val_accuracy: 0.5946\n",
            "Epoch 743/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.1973 - accuracy: 0.8914 - val_loss: 7.6405 - val_accuracy: 0.6126\n",
            "Epoch 744/1000\n",
            "28/28 [==============================] - 2s 79ms/step - loss: 0.2058 - accuracy: 0.8914 - val_loss: 8.5171 - val_accuracy: 0.6126\n",
            "Epoch 745/1000\n",
            "28/28 [==============================] - 2s 86ms/step - loss: 0.2298 - accuracy: 0.8857 - val_loss: 7.7081 - val_accuracy: 0.5991\n",
            "Epoch 746/1000\n",
            "28/28 [==============================] - 3s 110ms/step - loss: 0.2054 - accuracy: 0.8914 - val_loss: 6.4687 - val_accuracy: 0.5991\n",
            "Epoch 747/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2041 - accuracy: 0.8857 - val_loss: 6.7686 - val_accuracy: 0.6216\n",
            "Epoch 748/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2026 - accuracy: 0.8857 - val_loss: 6.9083 - val_accuracy: 0.6126\n",
            "Epoch 749/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2028 - accuracy: 0.8914 - val_loss: 7.0665 - val_accuracy: 0.6036\n",
            "Epoch 750/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2065 - accuracy: 0.8903 - val_loss: 7.1978 - val_accuracy: 0.6081\n",
            "Epoch 751/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2000 - accuracy: 0.8993 - val_loss: 7.2975 - val_accuracy: 0.6081\n",
            "Epoch 752/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2043 - accuracy: 0.8824 - val_loss: 7.2152 - val_accuracy: 0.6261\n",
            "Epoch 753/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2027 - accuracy: 0.8880 - val_loss: 7.3692 - val_accuracy: 0.6261\n",
            "Epoch 754/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2001 - accuracy: 0.8880 - val_loss: 7.3660 - val_accuracy: 0.6216\n",
            "Epoch 755/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2012 - accuracy: 0.8891 - val_loss: 7.5544 - val_accuracy: 0.6171\n",
            "Epoch 756/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2057 - accuracy: 0.8846 - val_loss: 7.7473 - val_accuracy: 0.6216\n",
            "Epoch 757/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.2026 - accuracy: 0.8778 - val_loss: 7.7747 - val_accuracy: 0.6126\n",
            "Epoch 758/1000\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.2057 - accuracy: 0.8891 - val_loss: 6.7754 - val_accuracy: 0.5991\n",
            "Epoch 759/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1999 - accuracy: 0.8903 - val_loss: 6.5413 - val_accuracy: 0.6126\n",
            "Epoch 760/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1999 - accuracy: 0.8903 - val_loss: 6.7891 - val_accuracy: 0.6261\n",
            "Epoch 761/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2010 - accuracy: 0.8891 - val_loss: 6.7010 - val_accuracy: 0.6036\n",
            "Epoch 762/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2011 - accuracy: 0.8880 - val_loss: 6.8754 - val_accuracy: 0.5901\n",
            "Epoch 763/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2019 - accuracy: 0.8869 - val_loss: 7.0364 - val_accuracy: 0.6216\n",
            "Epoch 764/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1972 - accuracy: 0.8925 - val_loss: 7.0645 - val_accuracy: 0.6216\n",
            "Epoch 765/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1992 - accuracy: 0.8880 - val_loss: 7.1479 - val_accuracy: 0.6126\n",
            "Epoch 766/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2005 - accuracy: 0.8914 - val_loss: 7.1839 - val_accuracy: 0.6081\n",
            "Epoch 767/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1989 - accuracy: 0.8846 - val_loss: 7.3287 - val_accuracy: 0.6126\n",
            "Epoch 768/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.1996 - accuracy: 0.8869 - val_loss: 7.3742 - val_accuracy: 0.6036\n",
            "Epoch 769/1000\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.2035 - accuracy: 0.8835 - val_loss: 7.2448 - val_accuracy: 0.6036\n",
            "Epoch 770/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2007 - accuracy: 0.8824 - val_loss: 7.2234 - val_accuracy: 0.6081\n",
            "Epoch 771/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2056 - accuracy: 0.8891 - val_loss: 7.1085 - val_accuracy: 0.6171\n",
            "Epoch 772/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1985 - accuracy: 0.8891 - val_loss: 7.2320 - val_accuracy: 0.6306\n",
            "Epoch 773/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2062 - accuracy: 0.8824 - val_loss: 7.2789 - val_accuracy: 0.5901\n",
            "Epoch 774/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2005 - accuracy: 0.8835 - val_loss: 7.3563 - val_accuracy: 0.5991\n",
            "Epoch 775/1000\n",
            "28/28 [==============================] - 2s 79ms/step - loss: 0.1999 - accuracy: 0.8857 - val_loss: 7.3620 - val_accuracy: 0.6081\n",
            "Epoch 776/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2009 - accuracy: 0.8857 - val_loss: 7.4467 - val_accuracy: 0.5946\n",
            "Epoch 777/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2022 - accuracy: 0.8857 - val_loss: 7.5377 - val_accuracy: 0.6036\n",
            "Epoch 778/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2004 - accuracy: 0.8869 - val_loss: 7.6047 - val_accuracy: 0.6081\n",
            "Epoch 779/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2230 - accuracy: 0.8846 - val_loss: 7.1256 - val_accuracy: 0.5991\n",
            "Epoch 780/1000\n",
            "28/28 [==============================] - 3s 101ms/step - loss: 0.2118 - accuracy: 0.8801 - val_loss: 6.7220 - val_accuracy: 0.5901\n",
            "Epoch 781/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2062 - accuracy: 0.8801 - val_loss: 6.7361 - val_accuracy: 0.6171\n",
            "Epoch 782/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2006 - accuracy: 0.8948 - val_loss: 6.9275 - val_accuracy: 0.6216\n",
            "Epoch 783/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2016 - accuracy: 0.8971 - val_loss: 6.9526 - val_accuracy: 0.6126\n",
            "Epoch 784/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2090 - accuracy: 0.8925 - val_loss: 6.6648 - val_accuracy: 0.6216\n",
            "Epoch 785/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.1987 - accuracy: 0.8982 - val_loss: 6.7132 - val_accuracy: 0.6261\n",
            "Epoch 786/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.2065 - accuracy: 0.8824 - val_loss: 7.1048 - val_accuracy: 0.6216\n",
            "Epoch 787/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2039 - accuracy: 0.8824 - val_loss: 7.6502 - val_accuracy: 0.6171\n",
            "Epoch 788/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2037 - accuracy: 0.8891 - val_loss: 7.8414 - val_accuracy: 0.6081\n",
            "Epoch 789/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1999 - accuracy: 0.8857 - val_loss: 8.0496 - val_accuracy: 0.5946\n",
            "Epoch 790/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2042 - accuracy: 0.8835 - val_loss: 7.8812 - val_accuracy: 0.5991\n",
            "Epoch 791/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.1978 - accuracy: 0.8903 - val_loss: 7.8903 - val_accuracy: 0.6036\n",
            "Epoch 792/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1999 - accuracy: 0.8914 - val_loss: 7.9020 - val_accuracy: 0.5811\n",
            "Epoch 793/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.2060 - accuracy: 0.8891 - val_loss: 7.3975 - val_accuracy: 0.5901\n",
            "Epoch 794/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2006 - accuracy: 0.8959 - val_loss: 7.2003 - val_accuracy: 0.6081\n",
            "Epoch 795/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1965 - accuracy: 0.8891 - val_loss: 7.4846 - val_accuracy: 0.6036\n",
            "Epoch 796/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2012 - accuracy: 0.8857 - val_loss: 7.8218 - val_accuracy: 0.5991\n",
            "Epoch 797/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.1966 - accuracy: 0.8982 - val_loss: 7.7857 - val_accuracy: 0.5946\n",
            "Epoch 798/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2047 - accuracy: 0.8824 - val_loss: 7.9174 - val_accuracy: 0.6126\n",
            "Epoch 799/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2006 - accuracy: 0.8824 - val_loss: 7.6212 - val_accuracy: 0.6081\n",
            "Epoch 800/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2037 - accuracy: 0.8846 - val_loss: 7.6443 - val_accuracy: 0.6171\n",
            "Epoch 801/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2070 - accuracy: 0.8857 - val_loss: 7.3895 - val_accuracy: 0.6261\n",
            "Epoch 802/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2013 - accuracy: 0.8925 - val_loss: 7.2088 - val_accuracy: 0.6261\n",
            "Epoch 803/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2014 - accuracy: 0.8824 - val_loss: 7.4150 - val_accuracy: 0.6126\n",
            "Epoch 804/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2014 - accuracy: 0.8869 - val_loss: 7.3725 - val_accuracy: 0.6036\n",
            "Epoch 805/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2004 - accuracy: 0.8891 - val_loss: 7.4697 - val_accuracy: 0.6126\n",
            "Epoch 806/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.1998 - accuracy: 0.8982 - val_loss: 7.4918 - val_accuracy: 0.5946\n",
            "Epoch 807/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.1980 - accuracy: 0.8891 - val_loss: 7.5656 - val_accuracy: 0.6081\n",
            "Epoch 808/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2025 - accuracy: 0.8971 - val_loss: 7.6827 - val_accuracy: 0.5946\n",
            "Epoch 809/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2023 - accuracy: 0.8869 - val_loss: 7.5601 - val_accuracy: 0.5991\n",
            "Epoch 810/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2068 - accuracy: 0.8891 - val_loss: 7.5044 - val_accuracy: 0.5766\n",
            "Epoch 811/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2046 - accuracy: 0.8824 - val_loss: 7.3418 - val_accuracy: 0.5811\n",
            "Epoch 812/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2012 - accuracy: 0.8869 - val_loss: 7.7163 - val_accuracy: 0.5811\n",
            "Epoch 813/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.1989 - accuracy: 0.8869 - val_loss: 7.8282 - val_accuracy: 0.5811\n",
            "Epoch 814/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2017 - accuracy: 0.8891 - val_loss: 7.7244 - val_accuracy: 0.5946\n",
            "Epoch 815/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.1963 - accuracy: 0.8914 - val_loss: 7.8855 - val_accuracy: 0.5856\n",
            "Epoch 816/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1968 - accuracy: 0.8880 - val_loss: 7.9613 - val_accuracy: 0.5811\n",
            "Epoch 817/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2023 - accuracy: 0.8937 - val_loss: 7.6815 - val_accuracy: 0.5766\n",
            "Epoch 818/1000\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.2013 - accuracy: 0.8880 - val_loss: 7.5399 - val_accuracy: 0.5901\n",
            "Epoch 819/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2017 - accuracy: 0.8835 - val_loss: 7.2968 - val_accuracy: 0.5901\n",
            "Epoch 820/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2008 - accuracy: 0.8891 - val_loss: 7.4305 - val_accuracy: 0.5856\n",
            "Epoch 821/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2009 - accuracy: 0.8925 - val_loss: 7.4467 - val_accuracy: 0.5856\n",
            "Epoch 822/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2001 - accuracy: 0.8824 - val_loss: 7.4731 - val_accuracy: 0.5901\n",
            "Epoch 823/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1981 - accuracy: 0.8857 - val_loss: 7.4870 - val_accuracy: 0.6036\n",
            "Epoch 824/1000\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.2008 - accuracy: 0.8857 - val_loss: 7.5544 - val_accuracy: 0.5991\n",
            "Epoch 825/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.1988 - accuracy: 0.8925 - val_loss: 7.6386 - val_accuracy: 0.5856\n",
            "Epoch 826/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2071 - accuracy: 0.8903 - val_loss: 7.6652 - val_accuracy: 0.5856\n",
            "Epoch 827/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2027 - accuracy: 0.8914 - val_loss: 7.6728 - val_accuracy: 0.6036\n",
            "Epoch 828/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2026 - accuracy: 0.8869 - val_loss: 7.5533 - val_accuracy: 0.6036\n",
            "Epoch 829/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2058 - accuracy: 0.8846 - val_loss: 6.9016 - val_accuracy: 0.5856\n",
            "Epoch 830/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.1975 - accuracy: 0.8959 - val_loss: 7.0567 - val_accuracy: 0.5946\n",
            "Epoch 831/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1988 - accuracy: 0.8869 - val_loss: 7.2478 - val_accuracy: 0.5991\n",
            "Epoch 832/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2005 - accuracy: 0.8835 - val_loss: 7.4146 - val_accuracy: 0.5901\n",
            "Epoch 833/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2008 - accuracy: 0.8812 - val_loss: 7.5709 - val_accuracy: 0.5946\n",
            "Epoch 834/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2027 - accuracy: 0.8846 - val_loss: 7.7101 - val_accuracy: 0.6171\n",
            "Epoch 835/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2024 - accuracy: 0.8857 - val_loss: 7.6025 - val_accuracy: 0.6216\n",
            "Epoch 836/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2033 - accuracy: 0.8846 - val_loss: 7.7137 - val_accuracy: 0.6171\n",
            "Epoch 837/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1965 - accuracy: 0.8925 - val_loss: 7.7566 - val_accuracy: 0.6126\n",
            "Epoch 838/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2020 - accuracy: 0.8903 - val_loss: 7.6805 - val_accuracy: 0.6081\n",
            "Epoch 839/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2033 - accuracy: 0.8790 - val_loss: 7.8035 - val_accuracy: 0.5946\n",
            "Epoch 840/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2043 - accuracy: 0.8857 - val_loss: 8.0557 - val_accuracy: 0.5991\n",
            "Epoch 841/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2005 - accuracy: 0.8948 - val_loss: 8.1583 - val_accuracy: 0.5856\n",
            "Epoch 842/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2030 - accuracy: 0.8880 - val_loss: 7.8697 - val_accuracy: 0.6216\n",
            "Epoch 843/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2026 - accuracy: 0.8880 - val_loss: 8.0525 - val_accuracy: 0.6261\n",
            "Epoch 844/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1991 - accuracy: 0.8948 - val_loss: 7.9933 - val_accuracy: 0.6216\n",
            "Epoch 845/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2029 - accuracy: 0.8857 - val_loss: 7.9071 - val_accuracy: 0.6126\n",
            "Epoch 846/1000\n",
            "28/28 [==============================] - 2s 86ms/step - loss: 0.2021 - accuracy: 0.8891 - val_loss: 7.5329 - val_accuracy: 0.6216\n",
            "Epoch 847/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.1998 - accuracy: 0.8869 - val_loss: 7.6350 - val_accuracy: 0.6081\n",
            "Epoch 848/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2007 - accuracy: 0.8914 - val_loss: 7.5613 - val_accuracy: 0.6216\n",
            "Epoch 849/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2014 - accuracy: 0.8914 - val_loss: 7.4461 - val_accuracy: 0.6081\n",
            "Epoch 850/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1979 - accuracy: 0.8925 - val_loss: 7.4441 - val_accuracy: 0.6126\n",
            "Epoch 851/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1986 - accuracy: 0.8857 - val_loss: 7.4879 - val_accuracy: 0.6171\n",
            "Epoch 852/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.2021 - accuracy: 0.8925 - val_loss: 7.5959 - val_accuracy: 0.6126\n",
            "Epoch 853/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2035 - accuracy: 0.8846 - val_loss: 7.6214 - val_accuracy: 0.6216\n",
            "Epoch 854/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1982 - accuracy: 0.8925 - val_loss: 7.5734 - val_accuracy: 0.6261\n",
            "Epoch 855/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2026 - accuracy: 0.8914 - val_loss: 7.5142 - val_accuracy: 0.6216\n",
            "Epoch 856/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2001 - accuracy: 0.8937 - val_loss: 7.5155 - val_accuracy: 0.6171\n",
            "Epoch 857/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1975 - accuracy: 0.8959 - val_loss: 7.9986 - val_accuracy: 0.5991\n",
            "Epoch 858/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.1987 - accuracy: 0.8846 - val_loss: 8.3066 - val_accuracy: 0.6036\n",
            "Epoch 859/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2038 - accuracy: 0.8880 - val_loss: 7.9925 - val_accuracy: 0.6081\n",
            "Epoch 860/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2025 - accuracy: 0.8824 - val_loss: 8.2209 - val_accuracy: 0.6036\n",
            "Epoch 861/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2033 - accuracy: 0.8756 - val_loss: 8.0486 - val_accuracy: 0.5901\n",
            "Epoch 862/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2004 - accuracy: 0.8835 - val_loss: 8.1134 - val_accuracy: 0.5946\n",
            "Epoch 863/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.2053 - accuracy: 0.8778 - val_loss: 7.5345 - val_accuracy: 0.6261\n",
            "Epoch 864/1000\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.2087 - accuracy: 0.8914 - val_loss: 8.2060 - val_accuracy: 0.6036\n",
            "Epoch 865/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2004 - accuracy: 0.8880 - val_loss: 8.4390 - val_accuracy: 0.6036\n",
            "Epoch 866/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2033 - accuracy: 0.8925 - val_loss: 8.1549 - val_accuracy: 0.5946\n",
            "Epoch 867/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2052 - accuracy: 0.8835 - val_loss: 8.0002 - val_accuracy: 0.6261\n",
            "Epoch 868/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2034 - accuracy: 0.8891 - val_loss: 7.9306 - val_accuracy: 0.6216\n",
            "Epoch 869/1000\n",
            "28/28 [==============================] - 3s 107ms/step - loss: 0.2075 - accuracy: 0.8857 - val_loss: 8.3476 - val_accuracy: 0.6126\n",
            "Epoch 870/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1996 - accuracy: 0.8880 - val_loss: 8.3340 - val_accuracy: 0.5946\n",
            "Epoch 871/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2084 - accuracy: 0.8790 - val_loss: 7.5254 - val_accuracy: 0.6081\n",
            "Epoch 872/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2013 - accuracy: 0.8880 - val_loss: 7.6316 - val_accuracy: 0.6261\n",
            "Epoch 873/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1966 - accuracy: 0.8948 - val_loss: 7.7928 - val_accuracy: 0.6126\n",
            "Epoch 874/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.1988 - accuracy: 0.8857 - val_loss: 7.9839 - val_accuracy: 0.6036\n",
            "Epoch 875/1000\n",
            "28/28 [==============================] - 3s 118ms/step - loss: 0.2017 - accuracy: 0.8903 - val_loss: 7.8927 - val_accuracy: 0.5946\n",
            "Epoch 876/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2026 - accuracy: 0.8846 - val_loss: 7.6579 - val_accuracy: 0.5991\n",
            "Epoch 877/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1998 - accuracy: 0.8903 - val_loss: 7.6593 - val_accuracy: 0.6036\n",
            "Epoch 878/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2021 - accuracy: 0.8846 - val_loss: 7.7540 - val_accuracy: 0.6081\n",
            "Epoch 879/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1988 - accuracy: 0.8925 - val_loss: 7.9521 - val_accuracy: 0.6171\n",
            "Epoch 880/1000\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.2004 - accuracy: 0.8846 - val_loss: 7.6992 - val_accuracy: 0.6081\n",
            "Epoch 881/1000\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.2019 - accuracy: 0.8959 - val_loss: 7.8800 - val_accuracy: 0.6081\n",
            "Epoch 882/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2008 - accuracy: 0.8869 - val_loss: 7.9841 - val_accuracy: 0.6081\n",
            "Epoch 883/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1992 - accuracy: 0.8959 - val_loss: 8.1409 - val_accuracy: 0.6171\n",
            "Epoch 884/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2043 - accuracy: 0.8857 - val_loss: 7.8789 - val_accuracy: 0.6126\n",
            "Epoch 885/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1976 - accuracy: 0.8948 - val_loss: 7.8742 - val_accuracy: 0.6216\n",
            "Epoch 886/1000\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.2025 - accuracy: 0.8925 - val_loss: 7.6462 - val_accuracy: 0.6216\n",
            "Epoch 887/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2018 - accuracy: 0.8824 - val_loss: 7.6364 - val_accuracy: 0.6216\n",
            "Epoch 888/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1998 - accuracy: 0.8948 - val_loss: 7.9345 - val_accuracy: 0.6216\n",
            "Epoch 889/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.1994 - accuracy: 0.8869 - val_loss: 8.0090 - val_accuracy: 0.6081\n",
            "Epoch 890/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2002 - accuracy: 0.8925 - val_loss: 8.1596 - val_accuracy: 0.6171\n",
            "Epoch 891/1000\n",
            "28/28 [==============================] - 2s 79ms/step - loss: 0.1977 - accuracy: 0.8925 - val_loss: 8.2462 - val_accuracy: 0.6216\n",
            "Epoch 892/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.2015 - accuracy: 0.8903 - val_loss: 8.0965 - val_accuracy: 0.6081\n",
            "Epoch 893/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2035 - accuracy: 0.8903 - val_loss: 8.1347 - val_accuracy: 0.6171\n",
            "Epoch 894/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2005 - accuracy: 0.8869 - val_loss: 8.2380 - val_accuracy: 0.6171\n",
            "Epoch 895/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2106 - accuracy: 0.8869 - val_loss: 7.4439 - val_accuracy: 0.6216\n",
            "Epoch 896/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2125 - accuracy: 0.8744 - val_loss: 7.2659 - val_accuracy: 0.6036\n",
            "Epoch 897/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.2037 - accuracy: 0.8846 - val_loss: 7.4919 - val_accuracy: 0.6126\n",
            "Epoch 898/1000\n",
            "28/28 [==============================] - 2s 86ms/step - loss: 0.2002 - accuracy: 0.8790 - val_loss: 7.7631 - val_accuracy: 0.6126\n",
            "Epoch 899/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2000 - accuracy: 0.8891 - val_loss: 7.8156 - val_accuracy: 0.5991\n",
            "Epoch 900/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2026 - accuracy: 0.8801 - val_loss: 7.6561 - val_accuracy: 0.6081\n",
            "Epoch 901/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1985 - accuracy: 0.8971 - val_loss: 7.6782 - val_accuracy: 0.6216\n",
            "Epoch 902/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2043 - accuracy: 0.8891 - val_loss: 7.7412 - val_accuracy: 0.6216\n",
            "Epoch 903/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.1989 - accuracy: 0.8914 - val_loss: 7.7833 - val_accuracy: 0.6126\n",
            "Epoch 904/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2033 - accuracy: 0.8846 - val_loss: 7.8164 - val_accuracy: 0.6171\n",
            "Epoch 905/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2017 - accuracy: 0.8925 - val_loss: 7.4971 - val_accuracy: 0.6171\n",
            "Epoch 906/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1991 - accuracy: 0.8891 - val_loss: 7.6882 - val_accuracy: 0.5991\n",
            "Epoch 907/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2044 - accuracy: 0.8812 - val_loss: 8.0856 - val_accuracy: 0.5991\n",
            "Epoch 908/1000\n",
            "28/28 [==============================] - 2s 88ms/step - loss: 0.2025 - accuracy: 0.8801 - val_loss: 8.1576 - val_accuracy: 0.6216\n",
            "Epoch 909/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2016 - accuracy: 0.8914 - val_loss: 8.0760 - val_accuracy: 0.6081\n",
            "Epoch 910/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2024 - accuracy: 0.8891 - val_loss: 8.4225 - val_accuracy: 0.6351\n",
            "Epoch 911/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2070 - accuracy: 0.8824 - val_loss: 8.2383 - val_accuracy: 0.6036\n",
            "Epoch 912/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1995 - accuracy: 0.8846 - val_loss: 7.9585 - val_accuracy: 0.6396\n",
            "Epoch 913/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2016 - accuracy: 0.8857 - val_loss: 7.9024 - val_accuracy: 0.6261\n",
            "Epoch 914/1000\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.1990 - accuracy: 0.8925 - val_loss: 8.0724 - val_accuracy: 0.6036\n",
            "Epoch 915/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2010 - accuracy: 0.8993 - val_loss: 7.9764 - val_accuracy: 0.6171\n",
            "Epoch 916/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2037 - accuracy: 0.8903 - val_loss: 7.9085 - val_accuracy: 0.6261\n",
            "Epoch 917/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1979 - accuracy: 0.8869 - val_loss: 8.0167 - val_accuracy: 0.6126\n",
            "Epoch 918/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2002 - accuracy: 0.8812 - val_loss: 8.1793 - val_accuracy: 0.6306\n",
            "Epoch 919/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1989 - accuracy: 0.8869 - val_loss: 8.3304 - val_accuracy: 0.6216\n",
            "Epoch 920/1000\n",
            "28/28 [==============================] - 3s 106ms/step - loss: 0.2038 - accuracy: 0.8857 - val_loss: 7.9836 - val_accuracy: 0.6306\n",
            "Epoch 921/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1996 - accuracy: 0.8880 - val_loss: 8.0823 - val_accuracy: 0.6396\n",
            "Epoch 922/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1962 - accuracy: 0.8971 - val_loss: 8.1828 - val_accuracy: 0.6036\n",
            "Epoch 923/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2072 - accuracy: 0.8891 - val_loss: 8.1369 - val_accuracy: 0.6036\n",
            "Epoch 924/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2078 - accuracy: 0.8869 - val_loss: 7.9209 - val_accuracy: 0.6396\n",
            "Epoch 925/1000\n",
            "28/28 [==============================] - 2s 88ms/step - loss: 0.2043 - accuracy: 0.8903 - val_loss: 8.0729 - val_accuracy: 0.6261\n",
            "Epoch 926/1000\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.2006 - accuracy: 0.8891 - val_loss: 8.4022 - val_accuracy: 0.6261\n",
            "Epoch 927/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2233 - accuracy: 0.8869 - val_loss: 7.4348 - val_accuracy: 0.6081\n",
            "Epoch 928/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2016 - accuracy: 0.8869 - val_loss: 6.8258 - val_accuracy: 0.6081\n",
            "Epoch 929/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2015 - accuracy: 0.8880 - val_loss: 7.0325 - val_accuracy: 0.6171\n",
            "Epoch 930/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2049 - accuracy: 0.8880 - val_loss: 7.0807 - val_accuracy: 0.6171\n",
            "Epoch 931/1000\n",
            "28/28 [==============================] - 3s 102ms/step - loss: 0.2063 - accuracy: 0.8790 - val_loss: 6.9896 - val_accuracy: 0.6126\n",
            "Epoch 932/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2011 - accuracy: 0.8903 - val_loss: 7.0454 - val_accuracy: 0.6036\n",
            "Epoch 933/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1992 - accuracy: 0.8925 - val_loss: 7.2999 - val_accuracy: 0.6036\n",
            "Epoch 934/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.1997 - accuracy: 0.8824 - val_loss: 7.3517 - val_accuracy: 0.5991\n",
            "Epoch 935/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2014 - accuracy: 0.8891 - val_loss: 7.2465 - val_accuracy: 0.6036\n",
            "Epoch 936/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.1995 - accuracy: 0.8891 - val_loss: 7.2963 - val_accuracy: 0.6036\n",
            "Epoch 937/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2000 - accuracy: 0.8857 - val_loss: 7.2641 - val_accuracy: 0.5991\n",
            "Epoch 938/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2000 - accuracy: 0.8869 - val_loss: 7.4781 - val_accuracy: 0.5991\n",
            "Epoch 939/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2022 - accuracy: 0.8925 - val_loss: 7.6203 - val_accuracy: 0.6126\n",
            "Epoch 940/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2008 - accuracy: 0.8857 - val_loss: 7.6523 - val_accuracy: 0.5991\n",
            "Epoch 941/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2026 - accuracy: 0.8903 - val_loss: 7.4241 - val_accuracy: 0.6081\n",
            "Epoch 942/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.2081 - accuracy: 0.8880 - val_loss: 7.0597 - val_accuracy: 0.6081\n",
            "Epoch 943/1000\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.2027 - accuracy: 0.8903 - val_loss: 7.0869 - val_accuracy: 0.6081\n",
            "Epoch 944/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1974 - accuracy: 0.8959 - val_loss: 7.0743 - val_accuracy: 0.6126\n",
            "Epoch 945/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2006 - accuracy: 0.8835 - val_loss: 7.1678 - val_accuracy: 0.6036\n",
            "Epoch 946/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1977 - accuracy: 0.8914 - val_loss: 7.3770 - val_accuracy: 0.6126\n",
            "Epoch 947/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2014 - accuracy: 0.8914 - val_loss: 7.4234 - val_accuracy: 0.6081\n",
            "Epoch 948/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.2014 - accuracy: 0.8846 - val_loss: 7.2317 - val_accuracy: 0.6081\n",
            "Epoch 949/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2129 - accuracy: 0.8812 - val_loss: 6.9212 - val_accuracy: 0.6081\n",
            "Epoch 950/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2032 - accuracy: 0.8857 - val_loss: 7.1423 - val_accuracy: 0.5991\n",
            "Epoch 951/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2068 - accuracy: 0.8880 - val_loss: 7.2142 - val_accuracy: 0.6171\n",
            "Epoch 952/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2005 - accuracy: 0.8925 - val_loss: 6.7266 - val_accuracy: 0.6351\n",
            "Epoch 953/1000\n",
            "28/28 [==============================] - 2s 82ms/step - loss: 0.1988 - accuracy: 0.8937 - val_loss: 6.7436 - val_accuracy: 0.6306\n",
            "Epoch 954/1000\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.2015 - accuracy: 0.8869 - val_loss: 6.8190 - val_accuracy: 0.6486\n",
            "Epoch 955/1000\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.2025 - accuracy: 0.8812 - val_loss: 7.0283 - val_accuracy: 0.6351\n",
            "Epoch 956/1000\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.2012 - accuracy: 0.8880 - val_loss: 7.1099 - val_accuracy: 0.6171\n",
            "Epoch 957/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1987 - accuracy: 0.8925 - val_loss: 8.1938 - val_accuracy: 0.6261\n",
            "Epoch 958/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2001 - accuracy: 0.8925 - val_loss: 8.4351 - val_accuracy: 0.6171\n",
            "Epoch 959/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.1998 - accuracy: 0.8846 - val_loss: 8.4610 - val_accuracy: 0.6306\n",
            "Epoch 960/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1999 - accuracy: 0.8891 - val_loss: 8.2348 - val_accuracy: 0.6351\n",
            "Epoch 961/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.1979 - accuracy: 0.8903 - val_loss: 7.9267 - val_accuracy: 0.6216\n",
            "Epoch 962/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2060 - accuracy: 0.8857 - val_loss: 7.7525 - val_accuracy: 0.6261\n",
            "Epoch 963/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.2034 - accuracy: 0.8846 - val_loss: 7.7599 - val_accuracy: 0.6351\n",
            "Epoch 964/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.1980 - accuracy: 0.8880 - val_loss: 7.5375 - val_accuracy: 0.6081\n",
            "Epoch 965/1000\n",
            "28/28 [==============================] - 2s 85ms/step - loss: 0.2023 - accuracy: 0.8891 - val_loss: 7.7341 - val_accuracy: 0.5991\n",
            "Epoch 966/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2027 - accuracy: 0.8824 - val_loss: 7.7690 - val_accuracy: 0.6036\n",
            "Epoch 967/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2019 - accuracy: 0.8869 - val_loss: 7.8928 - val_accuracy: 0.6081\n",
            "Epoch 968/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.1993 - accuracy: 0.8880 - val_loss: 7.8210 - val_accuracy: 0.6081\n",
            "Epoch 969/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2036 - accuracy: 0.8812 - val_loss: 7.1919 - val_accuracy: 0.6036\n",
            "Epoch 970/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2018 - accuracy: 0.8857 - val_loss: 7.5810 - val_accuracy: 0.6126\n",
            "Epoch 971/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2007 - accuracy: 0.8914 - val_loss: 8.1328 - val_accuracy: 0.5946\n",
            "Epoch 972/1000\n",
            "28/28 [==============================] - 2s 73ms/step - loss: 0.2041 - accuracy: 0.8812 - val_loss: 8.2959 - val_accuracy: 0.6036\n",
            "Epoch 973/1000\n",
            "28/28 [==============================] - 2s 72ms/step - loss: 0.2005 - accuracy: 0.8948 - val_loss: 8.3250 - val_accuracy: 0.6126\n",
            "Epoch 974/1000\n",
            "28/28 [==============================] - 2s 74ms/step - loss: 0.1973 - accuracy: 0.8925 - val_loss: 8.3462 - val_accuracy: 0.6036\n",
            "Epoch 975/1000\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.2042 - accuracy: 0.8835 - val_loss: 8.2582 - val_accuracy: 0.6081\n",
            "Epoch 976/1000\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.1977 - accuracy: 0.8880 - val_loss: 8.3293 - val_accuracy: 0.6171\n",
            "Epoch 977/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.2023 - accuracy: 0.8891 - val_loss: 8.2001 - val_accuracy: 0.6126\n",
            "Epoch 978/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1993 - accuracy: 0.8891 - val_loss: 8.1576 - val_accuracy: 0.6171\n",
            "Epoch 979/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2011 - accuracy: 0.8880 - val_loss: 8.0726 - val_accuracy: 0.6171\n",
            "Epoch 980/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2001 - accuracy: 0.8959 - val_loss: 8.1989 - val_accuracy: 0.6171\n",
            "Epoch 981/1000\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.2017 - accuracy: 0.8869 - val_loss: 8.0349 - val_accuracy: 0.6171\n",
            "Epoch 982/1000\n",
            "28/28 [==============================] - 3s 111ms/step - loss: 0.2050 - accuracy: 0.8925 - val_loss: 8.2551 - val_accuracy: 0.6261\n",
            "Epoch 983/1000\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.2083 - accuracy: 0.8846 - val_loss: 8.1314 - val_accuracy: 0.6306\n",
            "Epoch 984/1000\n",
            "28/28 [==============================] - 3s 110ms/step - loss: 0.2072 - accuracy: 0.8835 - val_loss: 8.3895 - val_accuracy: 0.6216\n",
            "Epoch 985/1000\n",
            "28/28 [==============================] - 3s 116ms/step - loss: 0.2016 - accuracy: 0.8857 - val_loss: 8.2439 - val_accuracy: 0.6171\n",
            "Epoch 986/1000\n",
            "28/28 [==============================] - 3s 94ms/step - loss: 0.1962 - accuracy: 0.8891 - val_loss: 8.3910 - val_accuracy: 0.6171\n",
            "Epoch 987/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.2009 - accuracy: 0.8937 - val_loss: 8.4816 - val_accuracy: 0.6261\n",
            "Epoch 988/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2026 - accuracy: 0.8835 - val_loss: 8.2910 - val_accuracy: 0.6306\n",
            "Epoch 989/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2005 - accuracy: 0.8869 - val_loss: 8.2190 - val_accuracy: 0.6261\n",
            "Epoch 990/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2036 - accuracy: 0.8914 - val_loss: 8.4253 - val_accuracy: 0.6306\n",
            "Epoch 991/1000\n",
            "28/28 [==============================] - 3s 105ms/step - loss: 0.2115 - accuracy: 0.8835 - val_loss: 8.0470 - val_accuracy: 0.6261\n",
            "Epoch 992/1000\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.2030 - accuracy: 0.8869 - val_loss: 7.4645 - val_accuracy: 0.6081\n",
            "Epoch 993/1000\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.1997 - accuracy: 0.8903 - val_loss: 7.5991 - val_accuracy: 0.6036\n",
            "Epoch 994/1000\n",
            "28/28 [==============================] - 2s 76ms/step - loss: 0.2002 - accuracy: 0.8982 - val_loss: 7.9411 - val_accuracy: 0.5991\n",
            "Epoch 995/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.1997 - accuracy: 0.8869 - val_loss: 7.9949 - val_accuracy: 0.6036\n",
            "Epoch 996/1000\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.2019 - accuracy: 0.8891 - val_loss: 7.8161 - val_accuracy: 0.5946\n",
            "Epoch 997/1000\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.2042 - accuracy: 0.8880 - val_loss: 7.8269 - val_accuracy: 0.5901\n",
            "Epoch 998/1000\n",
            "28/28 [==============================] - 2s 78ms/step - loss: 0.2042 - accuracy: 0.8903 - val_loss: 7.9977 - val_accuracy: 0.5766\n",
            "Epoch 999/1000\n",
            "28/28 [==============================] - 2s 79ms/step - loss: 0.1994 - accuracy: 0.8925 - val_loss: 8.0955 - val_accuracy: 0.5991\n",
            "Epoch 1000/1000\n",
            "28/28 [==============================] - 2s 75ms/step - loss: 0.1999 - accuracy: 0.8824 - val_loss: 8.3942 - val_accuracy: 0.5946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e2d879",
      "metadata": {
        "id": "e6e2d879",
        "outputId": "c9df783b-c3d8-40fe-dd21-bde09373af5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mendapatkan loss dan akurasi pada data latih dan data uji\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Menghasilkan plot untuk loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Menghasilkan plot untuk akurasi\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan prediksi pada data test\n",
        "prediksi = model.predict(test_padded)\n",
        "\n",
        "# Mengkonversi hasil prediksi menjadi label kategori\n",
        "label_prediksi = np.argmax(prediksi, axis=1)\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "for i in range(len(test_padded)):\n",
        "    print(f\"Data test ke-{i+1}: {test_padded[i]}\")\n",
        "    print(f\"Prediksi: {label_prediksi[i]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7S2TUBwxtfE",
        "outputId": "4bd6847b-a41b-47c9-e4dc-13df0c820afa"
      },
      "id": "f7S2TUBwxtfE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 11ms/step\n",
            "Data test ke-1: [134  27   1 620   1 735   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-2: [238  53 295  10  18 296   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-3: [ 11 118  57 294  32   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-4: [100 175 162  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-5: [  1 249  22  39 307   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-6: [ 15  12 173  84  17   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-7: [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Prediksi: 1\n",
            "Data test ke-8: [36  9 22 31  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-9: [723 459   8  86   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-10: [  8 279 372 257   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-11: [ 36   9 329  55 234 906   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-12: [ 15  12 173  84  17   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-13: [190   1 586   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 6\n",
            "Data test ke-14: [ 15  12   3 193 176   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-15: [156  88 499  76  53 897   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-16: [ 18 450   1  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-17: [ 44  30   5 104   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-18: [82 83 45 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 0\n",
            "Data test ke-19: [45  9  3  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 0\n",
            "Data test ke-20: [ 11 953  55 954  66 366   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-21: [  1  96 318   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-22: [ 5 10 18 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-23: [ 2  4 28  1 41  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-24: [196 298   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 8\n",
            "Data test ke-25: [ 62 206  95  96  97   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-26: [175 212 269   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-27: [ 43  32  84 459  17   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-28: [169   5  16  13  14   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-29: [  5  88 495 125   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-30: [781 470 376   1  51   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-31: [ 46  23   4 343   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-32: [45  9 82 83 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 5\n",
            "Data test ke-33: [ 66  21 151   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-34: [ 45   9 543  25   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-35: [  1 152   3  53  92 208 453  10 165   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-36: [15 12  3  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 4\n",
            "Data test ke-37: [ 48 333 880   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-38: [ 89 603   2  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-39: [836   5 232  10  18  21   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-40: [ 3 30  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 1\n",
            "Data test ke-41: [156 155 585 217   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-42: [ 51  85  50 192   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-43: [ 51  85  50 192   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-44: [  3 132 133   7   5   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-45: [237 123   7   3  31   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-46: [153 152 187 821   1 452 453   1 451 621 793   1   1   7 621   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-47: [45  9  3  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 0\n",
            "Data test ke-48: [  6  70 245   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-49: [553 285   6  20 374   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-50: [347 259  70   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-51: [ 22   1   1 249 309   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-52: [156   1   1   1   1  34  38   1 390   1   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-53: [190   1   1   1   1 337   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-54: [  1  52 182   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-55: [175 222   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-56: [154  43 335  51   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-57: [  3   6 123   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-58: [ 22  39  49 227 282   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-59: [ 70  37 533   4  80   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-60: [  3  28 214   6 231   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-61: [433 434  93 102 108   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-62: [272   1 142  73   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-63: [ 24 534  10  18  21   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-64: [ 1  1  7 69  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-65: [ 20   4  68 130 105   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-66: [378   1   2   1 107   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-67: [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Prediksi: 1\n",
            "Data test ke-68: [787 723 459   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-69: [839 840 841  15  12   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-70: [ 71  38  81 139   7  99   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-71: [  8 279 372 257   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-72: [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Prediksi: 1\n",
            "Data test ke-73: [949 950 504   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 7\n",
            "Data test ke-74: [46  1 87  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 5\n",
            "Data test ke-75: [293 262  72  17   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-76: [ 1 22  1  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-77: [9 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Prediksi: 2\n",
            "Data test ke-78: [750  23  25   1  17   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-79: [305 131   5 963   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-80: [650 242 183   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-81: [ 25  96 200 153   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-82: [329 234   1 438   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-83: [232   1   1  94 757   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-84: [42 79 35 11  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-85: [20  4 90  6 19 29  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 1\n",
            "Data test ke-86: [738 179  65 197   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-87: [15 12 50 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 4\n",
            "Data test ke-88: [127  56 128  88   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-89: [682 683   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-90: [  1 620   1  30  58   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-91: [319  39 307  16  13  14   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-92: [  3  33   1 447  17   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-93: [127  56 128  91   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-94: [175 100 278  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-95: [ 70  37 533   4  80   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-96: [47  1 26  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-97: [406   1  30  58   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-98: [40 59  2 92  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-99: [ 66 129  57 130   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-100: [787 723 459   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-101: [179 633 634   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-102: [ 20   4  68   3   6 105   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-103: [ 7 44  5 10 18 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 3\n",
            "Data test ke-104: [ 46 271  65   7  99   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-105: [154  43 335  51   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-106: [215 499 956  11  54   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-107: [ 1 30 58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 1\n",
            "Data test ke-108: [603 310 311 549   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-109: [134  27  46 106  41 205   9   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-110: [ 36 222  11   3  54   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-111: [  1   1 381 712   1   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-112: [349  96 318 200 153   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-113: [ 26 163  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-114: [ 33 306 300   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-115: [175 201   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-116: [ 91  31 226   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-117: [  1 156   1 745 191 801   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-118: [ 38   1   1 101   1 966  23   1   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-119: [737  52  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-120: [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Prediksi: 1\n",
            "Data test ke-121: [ 3 44  5 10 18 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 3\n",
            "Data test ke-122: [46  1 87  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 5\n",
            "Data test ke-123: [  3  52 301   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-124: [ 48 149   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-125: [ 40  59   2  34 114   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-126: [168 115 116 117   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-127: [723 459   1 243 798   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-128: [ 25 145 273   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-129: [ 7 44  5 10 18 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 3\n",
            "Data test ke-130: [  2   4  27 474 673  76   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-131: [ 48 333 147   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-132: [ 46 271  65   7  99   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-133: [ 48  27 256 257   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-134: [  8  37  80 394   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-135: [30 58  1 33  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 1\n",
            "Data test ke-136: [839 840 841  15  12   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-137: [  1 381 404 380   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-138: [323 455 456  20  38  24   1   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-139: [347 259  70   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-140: [  8 279 372 257   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-141: [  1   1  96 318   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-142: [480 310 311 855  41 146  32   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-143: [373  47  44  10  18   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-144: [433 434   1  64   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-145: [ 3  6 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 1\n",
            "Data test ke-146: [319  39 307  16  13  14   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-147: [805   9 129  57 130   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-148: [ 88 865 866   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-149: [26  5  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-150: [179   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-151: [283 137 284  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-152: [ 42 881   7 451   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-153: [ 1  1  1 41  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-154: [373 172  23  26   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-155: [147 245   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-156: [ 11  47  43 900   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-157: [196 298   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 8\n",
            "Data test ke-158: [319  39 307  16  13  14   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-159: [  8 279   7  61   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-160: [583  30   5  34   1 823   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-161: [889  84 271   1 509   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-162: [ 48 924   3  55  25   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-163: [109 110   1  55   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-164: [  2  68 154   7   1   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-165: [  3 121 188 223  18   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-166: [619 324 928   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-167: [580   2   4 138  10  37   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-168: [  1 788 419 310 311 735   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-169: [283 137 284  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-170: [ 20   4  27 927 324 281   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-171: [ 46  65 143   7   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-172: [  7 390 242  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-173: [289 357 256 766 268  38   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-174: [ 48 880  95   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-175: [ 88 390  52 415   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-176: [319  39 307  16  13  14   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-177: [ 26  52 256 898 899   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-178: [735 497   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-179: [157 136 504  53 720   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 7\n",
            "Data test ke-180: [16 13 14 70  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 4\n",
            "Data test ke-181: [20  4 60 22 54 19 29  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 0\n",
            "Data test ke-182: [ 25  98   1 444   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-183: [727 136   1  24   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 7\n",
            "Data test ke-184: [ 71 134  28 139 219   7  99   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-185: [ 71  38  81 139   7  99   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-186: [  2 140  70 185  63   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 4\n",
            "Data test ke-187: [  8 151 218   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-188: [ 75 872   1  51   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-189: [ 2  4 28  1 41  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-190: [  1 329 234   1   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-191: [40 59  2 92  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 2\n",
            "Data test ke-192: [ 89 603   2  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-193: [ 80  85  72 609   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-194: [  5 220 274 125   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-195: [583  30  69  34   1   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-196: [405 179  65 197   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-197: [ 11 953  55 954  66 366   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-198: [793   1  30  58   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-199: [  2   4  28 402 325 326   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-200: [ 48 333 769   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-201: [ 74 180 202  26   3   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-202: [  2   4  28 402 325 326   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-203: [ 7 44  5 10 18 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 3\n",
            "Data test ke-204: [  1 501 502  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n",
            "Data test ke-205: [  7  30 104   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 3\n",
            "Data test ke-206: [125   5   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-207: [ 48 194 201 149   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-208: [ 24 606   1 607   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-209: [416 119   6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-210: [406   9  11   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-211: [787 103 788   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-212: [ 74 180 202  26   3   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-213: [373 172  23   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-214: [124 414 308 168 632 168   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-215: [196 298   1 875   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 8\n",
            "Data test ke-216: [45  9 82 83 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Prediksi: 5\n",
            "Data test ke-217: [ 47  69 207  10  18  21   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 1\n",
            "Data test ke-218: [157 136 399  24 637   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 7\n",
            "Data test ke-219: [ 48 149 564   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 2\n",
            "Data test ke-220: [  1   1   1 354 186   1   1   1   1 156   1   1 516   1   1   1   1   1\n",
            " 100   7   0]\n",
            "Prediksi: 3\n",
            "Data test ke-221: [  1   1   1 690  27   1   1   1   1   1   1 299 381 801  28   1  24  25\n",
            "   0   0   0]\n",
            "Prediksi: 5\n",
            "Data test ke-222: [ 26 163  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "Prediksi: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan evaluasi model pada data test\n",
        "loss, accuracy = model.evaluate(test_padded, y_test_final)\n",
        "\n",
        "# Menampilkan akurasi model\n",
        "print(\"Akurasi pada data test: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKT2b_Gr3o91",
        "outputId": "551e9a69-2885-445f-86b5-41d557882f98"
      },
      "id": "iKT2b_Gr3o91",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 11ms/step - loss: 3.9634 - accuracy: 0.5721\n",
            "Akurasi pada data test:  0.5720720887184143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan metode \"loc\" untuk melakukan filtering berdasarkan kategori\n",
        "kategori_tertentu = 5  # Ganti dengan kategori yang diinginkan\n",
        "produk_kategori_tertentu = data_train.loc[data_train[5] == kategori_tertentu]\n",
        "\n",
        "# Menampilkan semua data dengan kategori tertentu yang sudah ditraining\n",
        "print(produk_kategori_tertentu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "AB3HV3Y5zi5y",
        "outputId": "0b8430af-39ab-4023-e902-4f155e305b08"
      },
      "id": "AB3HV3Y5zi5y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 5",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9a1ba61c4460>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Menggunakan metode \"loc\" untuk melakukan filtering berdasarkan kategori\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkategori_tertentu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;31m# Ganti dengan kategori yang diinginkan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mproduk_kategori_tertentu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkategori_tertentu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Menampilkan semua data dengan kategori tertentu yang sudah ditraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 5"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}